{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Backpropagation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi7Rzl60Cjey"
      },
      "source": [
        "# 오차역전파법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9URVegj7H2Xh"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQEpD8xZ6miI"
      },
      "source": [
        "# 단순한 계층 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H1mpAR26sID"
      },
      "source": [
        "곱셈 계층 먼저 구현 \n",
        "\n",
        "- 순전파 : 입력을 *\n",
        "\n",
        "- 역전파 : 상류에서 넘어온 미분 값에 순전파 값을 서로 바꾸고 *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qcs08A5Benb"
      },
      "source": [
        "# 곱셈 계층\n",
        "class MulLayer(object):\n",
        "    def __init__(self): # 인스턴스 변수 x 와 y 를 초기화\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "        \n",
        "    def forward(self, x, y): # 순전파 시 인스턴스 변수 x와 y에 값이 들어감.\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x * y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout): # 역전파\n",
        "        dx = dout * self.y # 상류의 값에 x와 y를 바꿔서 곱한다.\n",
        "        dy = dout * self.x\n",
        "\n",
        "        return dx, dy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNvW6aJ067wn",
        "outputId": "fe46c5f1-5a78-4417-fea9-239db64763c9"
      },
      "source": [
        "# 곱셈 계층의 순전파\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "# 계층들을 객체로\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# 순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num) # 순수 사과 가격\n",
        "price = mul_tax_layer.forward(apple_price, tax) # 세금 포함 가격\n",
        "\n",
        "print(apple_price)\n",
        "print(price)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "220.00000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTPcm-oi7c1i",
        "outputId": "56283c6a-e26c-4352-eab3-cb4af1718ff2"
      },
      "source": [
        "# 역전파\n",
        "dprice = 1 # cost에 대한 처음 미분 값 당연히 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "print(dapple_price, dtax)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "print(dapple, dapple_num)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1 200\n",
            "2.2 110.00000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLFUEg-r8Ta5"
      },
      "source": [
        "덧셈 계층 구현\n",
        "\n",
        "- 순전파 : 그냥 더해주면 끝\n",
        "- 역전파 : 상류에서 온 미분 값 그대로 반환하면 끝"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIJCP3Mj8Stl"
      },
      "source": [
        "# 덧셈 계층 구현\n",
        "class AddLayer(object):\n",
        "    def __init__(self): # 덧셈 계층은 저장해놓고 쓸 인스턴스 변수가 없다.\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        out = x + y\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout*1 # 반환은 두개 해줘야 함\n",
        "        dy = dout*1\n",
        "        return dx, dy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGET-q-P9lv8",
        "outputId": "f6dd952e-ba63-433c-bec8-92be3e851e93"
      },
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# 계층들\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# 순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num) #(1)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num) #(2)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)#(3)\n",
        "price = mul_tax_layer.forward(all_price,tax)#(4)\n",
        "print(\"< 순전파 결과(총 가격) >\")\n",
        "print(price)\n",
        "\n",
        "print(\"< 역전파 결과 순서대로 >\")\n",
        "# 역전파\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice) #(4)\n",
        "print(dall_price, dtax)\n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) #(3)\n",
        "print(dapple_price, dorange_price)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price) #(2)\n",
        "print(dorange, dorange_num)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)\n",
        "print(dapple, dapple_num)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< 순전파 결과(총 가격) >\n",
            "715.0000000000001\n",
            "< 역전파 결과 순서대로 >\n",
            "1.1 650\n",
            "1.1 1.1\n",
            "3.3000000000000003 165.0\n",
            "2.2 110.00000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRaliwETHBw0"
      },
      "source": [
        "# 활성화 함수 계층 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyDZk6QtCZ8G"
      },
      "source": [
        "ReLU 계층\n",
        "\n",
        "ReLU 수식\n",
        "\n",
        "$y = \\begin{cases}x&(x > 0)\\\\0&(x \\le 0)\\end{cases}​$\n",
        "\n",
        "x에 대한 y의 미분\n",
        "\n",
        "$\\frac{\\partial{y}}{\\partial{x}} = \\begin{cases}1&(x > 0)\\\\0&(x \\le 0)\\end{cases}​$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h54symCPHEJ7"
      },
      "source": [
        "class Relu(object):\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0) # 0이하의 값만 True\n",
        "        out = x.copy() # 입력을 그대로 받아서\n",
        "        out[self.mask] = 0 # 0이하의 값은 0으로 바꿔준다\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(sefl,dout): # 0보다 크면 상류 흐름 그대로 0보다 이하면 0\n",
        "        dout[self.mask] = 0 # 0이하의 값만 0으로 바꿔준다\n",
        "        dx = dout \n",
        "\n",
        "        return dx"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pyh0aqZHoTA",
        "outputId": "40e488d1-6a09-4f28-a7d5-8ae0eca187b7"
      },
      "source": [
        "x = np.array( [[1.0, -0.5], [-2.0, 3.0]])\n",
        "print(x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.  -0.5]\n",
            " [-2.   3. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlaK2pk2H66L",
        "outputId": "ff1bff7f-2e42-4dcb-f8a2-6066730d8c12"
      },
      "source": [
        "mask = (x <= 0)\n",
        "print(mask)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[False  True]\n",
            " [ True False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toGsFLffH9U4",
        "outputId": "e0f97e33-79de-4c40-833a-2eed159a4018"
      },
      "source": [
        "out = x.copy()\n",
        "print(out[mask])\n",
        "\n",
        "out[mask] = 0 # True에 해당하는 원소만 0으로 바꿈\n",
        "print(out) # 0 이하의 값은 0으로 변경됐음"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.5 -2. ]\n",
            "[[1. 0.]\n",
            " [0. 3.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l1p2DPcEDXM"
      },
      "source": [
        "Sigmoid 계층\n",
        "\n",
        "$y = \\frac{1}{1+e^{-x}}$\n",
        "\n",
        "x에 대해 y를 미분하면\n",
        "\n",
        "$\\frac{\\partial{y}}{\\partial{x}} = y(1-y)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da669dRyECjT"
      },
      "source": [
        "class Sigmoid(object):\n",
        "    def __init__(self):\n",
        "        self.out = None # 인스턴스 변수 Out (위 식에서 y에 해당)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x)) # 그냥 순전파\n",
        "        self.out = out # 역전파때 사용할 y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out # y를 그대로 다시 사용\n",
        "\n",
        "        return dx"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtmuaTE2F5m_"
      },
      "source": [
        "# Affine / Softmax 계층 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0lTNkyYF4lM"
      },
      "source": [
        "class Affine(object):\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.x = None # 순전파시 입력 x를 담아 둘 인스턴스 변수\n",
        "        self.dW = None # 역전파시 Loss를 W로 편미분한 값\n",
        "        self.db = None # 역전파시 Loss를 b로 편미분한 값\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self,dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout) # X_transpose * dout\n",
        "        self.db = np.sum(dout, axis = 0) \n",
        "\n",
        "        return dx"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ue47ZptYCox"
      },
      "source": [
        "신경망에서 수행하는 작업은 두가지 1. 학습  2. 추론 이 있다.\n",
        "\n",
        "아래는 참고만, 상황마다 다릅니다.\n",
        "\n",
        "- 추론에는 softmax 함수 같은 활성화 함수를 사용하지 않음. Affine 계층에서 나온 결과 값을 그대로 사용하는 것이 일반적이고, 그 값을 score라고 부른다.\n",
        "\n",
        "- 학습에는 softmax 계층과 같은 활성화 함수를 사용하는데, 정규화한 출력 값을 이용해서 모델을 다시 업데이트해야 하기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrKmcW5EIRxq"
      },
      "source": [
        "def softmax(a): \n",
        "    c = np.max(a) \n",
        "    exp_a = np.exp(a) \n",
        "    sum_exp_a = np.sum(exp_a) \n",
        "    y = exp_a / sum_exp_a \n",
        "    return y\n",
        "\n",
        "class SoftmaxWithLoss(object):\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실\n",
        "        self.y = None # softmax의 출력\n",
        "        self.t = None # 정답 레이블 (one-hot-vector)\n",
        "\n",
        "    \n",
        "    def forward(self, x, t):\n",
        "        self.t = t # 역전파 때 사용해야 하므로 인스턴스 변수에 저장\n",
        "        self.y = softmax(x) \n",
        "        self.loss = cross_entropy_error(self.y, self.t) \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0] # batch의 크기\n",
        "        dx = (self.y - self.t) / batch_size \n",
        "        \n",
        "        return dx"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO4n5JNFXYdo"
      },
      "source": [
        "# 오차역전파법 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipzm-KsYXbid"
      },
      "source": [
        "정확히는 오차역전파를 활용한 신경망 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKBu4g7qXa_F"
      },
      "source": [
        "path = '/content/drive/MyDrive/deep-learning-from-scratch-master'\n",
        "os.chdir(path)\n",
        "sys.path.append(os.chdir)\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet(object):\n",
        "    #순서대로 입력층, 은닉층, 출력층, 가중치 초기화 시 정규분포의 스케일\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01): # 생성자, 초기화를 수행\n",
        "        # 가중치 초기화\n",
        "        self.params = {}   # 딕셔너리 변수로 신경망의 parameter를 보관\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)  # 1번째 층의 가중치\n",
        "        self.params['b1'] = np.zeros(hidden_size)                                       # 1번째 층의 편향\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) # 2번째 층의 가중치\n",
        "        self.params['b2'] = np.zeros(output_size)                                       # 2번째 층의 편향\n",
        "\n",
        "        self.layers = OrderedDict()  # 순서가있는 딕셔너리 변수, 신경망의 게층을 보관\n",
        "        # 아래 layer들을 순서대로 layers라는 딕셔너리에 저장\n",
        "        # 순전파 때는 추가한 순서대로 forward 메서드를 호출, 역전파 때는 반대 순서로 backward 메서드를 호출\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])   # 첫번째 Affine layer\n",
        "        self.layers['Relu1'] = Relu()                                           # Relu\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])   # 두번째 Affine layer\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()         # 신경망에서의 마지막 계층 ==> 여기서는 SoftmaxWithLoss\n",
        "        \n",
        "\n",
        "    def predict(self, x):   # 추론\n",
        "        for layer in self.layers.values(): # Affine2까지\n",
        "            x = layer.forward(x)\n",
        "        \n",
        "        return x # score\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):  # 손실\n",
        "        y = self.predict(x) \n",
        "        return self.lastLayer.forward(y, t) # softmax with loss의 결과\n",
        "    \n",
        "    def accuracy(self, x, t):  # 정확도\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1) \n",
        "        \n",
        "        if t.ndim != 1 : # t가 1차원이 아닐때 => 원 핫 인코딩\n",
        "            t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0]) # 배치 사이즈로 나눠 준다\n",
        "        return accuracy \n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):  # 가중치 매개변수의 기울기를 수치 미분 방식으로 구한다. (앞에서 했는 내용)\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t): # 가중치 매개변수의 기울기를 오차 역전파법으로 구한다.\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'] = self.layers['Affine1'].dW\n",
        "        grads['b1'] = self.layers['Affine1'].db\n",
        "        grads['W2'] = self.layers['Affine2'].dW\n",
        "        grads['b2'] = self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_tD36UlaU8t"
      },
      "source": [
        "오차역전파법을 사용해 구현한 신경망을 사용해보자.\n",
        "\n",
        "batch size가 조금만 커져도 numerical_gradient가 너무 느려서 3으로 잡음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvZ5sjs-aUYL",
        "outputId": "37c7cdc2-cf4b-4f44-f267-f48db7dbbdd2"
      },
      "source": [
        "from dataset.mnist import load_mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
        "\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
        "\n",
        "# batch_size = 3\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "for key in grad_numerical.keys(): # 각 가중치 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
        "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
        "    print(key + \":\" + str(diff))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1:9.336906448110266e-09\n",
            "b1:4.229335754201357e-07\n",
            "W2:4.631309440171019e-09\n",
            "b2:1.400932915451847e-07\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}