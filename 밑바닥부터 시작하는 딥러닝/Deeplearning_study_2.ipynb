{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝 스터디 2권",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jgph7N2dd5m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a795afa0-a1b6-4a14-f5c6-f9aba342f4f1"
      },
      "source": [
        "# 2권\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3])\n",
        "print(x.__class__)   # 클래스 이름 표시\n",
        "print(x.shape)\n",
        "print(x.ndim)\n",
        "\n",
        "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(W.shape)\n",
        "print(W.ndim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(3,)\n",
            "1\n",
            "(2, 3)\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgzTJiXvdhLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b86307a-3d2a-47cf-ef43-d6f7d5f125b9"
      },
      "source": [
        "# 행렬의 원소별 연산\n",
        "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "X = np.array([[0, 1, 2], [3, 4, 5]])\n",
        "\n",
        "print(W + X, '\\n')\n",
        "print(W * X, '\\n')\n",
        "\n",
        "# 브로드캐스트\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "print(A * 10, '\\n')\n",
        "\n",
        "b = np.array([10, 20])\n",
        "print(A * b, '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  3  5]\n",
            " [ 7  9 11]] \n",
            "\n",
            "[[ 0  2  6]\n",
            " [12 20 30]] \n",
            "\n",
            "[[10 20]\n",
            " [30 40]] \n",
            "\n",
            "[[10 40]\n",
            " [30 80]] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z1q0GA5dhRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2631b07e-15e8-4add-8928-56ef0a64ed83"
      },
      "source": [
        "# 벡터의 내적\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "a1 = np.dot(a, b)\n",
        "\n",
        "print(a1)\n",
        "\n",
        "# 행렬의 곱\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "a2 = np.matmul(A, B)\n",
        "\n",
        "print(a2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "[[19 22]\n",
            " [43 50]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isuNt4VidhZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1796fd18-6c26-4387-880e-e964ac1e8d6c"
      },
      "source": [
        "# 완전연결계층에 의한 변한의 미니배치 버전\n",
        "\n",
        "import numpy as np\n",
        "W1 = np.random.randn(2, 4) # 가중치\n",
        "b1 = np.random.randn(4)   # 편향\n",
        "x = np.random.randn(10 ,2) # 입력\n",
        "h = np.matmul(x, W1) + b1\n",
        "\n",
        "print(h.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBR4W2WkdhfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004c130a-a0e8-4257-87d1-755b1f245871"
      },
      "source": [
        "# 완전연결계층에 의한 변환에 활성화 함수를 사용 (선형 -> 비선형)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.random.randn(10, 2)\n",
        "W1 = np.random.randn(2, 4)\n",
        "b1 = np.random.randn(4)\n",
        "W2 = np.random.randn(4, 3)\n",
        "b2 = np.random.randn(3)\n",
        "\n",
        "h = np.matmul(x, W1) + b1\n",
        "a = sigmoid(h)\n",
        "s = np.matmul(a, W2) + b2\n",
        "\n",
        "print(h.shape)\n",
        "print(a.shape)\n",
        "print(s.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 4)\n",
            "(10, 4)\n",
            "(10, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y03c-7xqdhkU"
      },
      "source": [
        "# Sigmoid, Affine 계층 순전파 구현\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params = []\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params\n",
        "        out = np.matmul(x, W) + b\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jat6ghAIdhpc"
      },
      "source": [
        "# TwoLayerNet의 구현\n",
        "\n",
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        I, H, O = input_size, hidden_size, output_size\n",
        "\n",
        "        # 가중치와 편향 초기화\n",
        "        W1 = np.random.randn(I, H)\n",
        "        b1 = np.random.randn(H)\n",
        "        W2 = np.random.randn(H, O)\n",
        "        b2 = np.random.randn(O)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = [\n",
        "            Affine(W1, b1),\n",
        "            Sigmoid(),\n",
        "            Affine(W2, b2)\n",
        "        ]\n",
        "\n",
        "        # 모든 가중치를 리스트에 모은다.\n",
        "        self.params = []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1fciI2NdhuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa39fd3-c25d-43a3-b1b3-ec54c1a574c3"
      },
      "source": [
        "x = np.random.randn(10, 2)\n",
        "model = TwoLayerNet(2, 4, 3)\n",
        "s = model.predict(x)\n",
        "print(s)\n",
        "print(s.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.431  1.697 -1.065]\n",
            " [ 0.748  1.779 -1.088]\n",
            " [ 0.623  2.157 -1.499]\n",
            " [ 0.506  2.177 -1.57 ]\n",
            " [ 1.439  1.994 -1.172]\n",
            " [ 0.603  1.653 -1.001]\n",
            " [ 0.593  2.171 -1.532]\n",
            " [ 0.99   2.218 -1.461]\n",
            " [-0.036  1.619 -1.073]\n",
            " [ 0.608  2.126 -1.459]]\n",
            "(10, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMWMPEhldhzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fdbffdf-c58c-4f73-89f4-e51fb1cbfb3e"
      },
      "source": [
        "# Repeat 노드 예시\n",
        "\n",
        "import numpy as np\n",
        "D, N = 8, 7\n",
        "x = np.random.randn(1, D)   # 입력\n",
        "y = np.repeat(x, N, axis = 0)  # 순전파 , np.repeat은 원소 복제를 수행하는 메서드\n",
        "\n",
        "dy = np.random.randn(N, D)  # 무작위 기울기\n",
        "dx = np.sum(dy, axis = 0, keepdims = True) # 역전파  keepdims는 처음에 입력 x가 (1,8) 형태니까 역전파해도 (1,8) 나오게 해주려는거같음. (2차원형태 유지)\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape, '\\n')\n",
        "\n",
        "print(dy.shape)\n",
        "print(dx.shape)\n",
        "print(dx.ndim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 8)\n",
            "(7, 8) \n",
            "\n",
            "(7, 8)\n",
            "(1, 8)\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smiRnMomdh4n"
      },
      "source": [
        "# MatMul 계층 구현\n",
        "\n",
        "class MatMul:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.dot(x, W)\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        self.grads[0][...] = dW\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3vNq_Uudh90"
      },
      "source": [
        "# Sigmoid 계층 구현\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASK6MM_leTlp"
      },
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params\n",
        "        out = np.dot(x, W) + b\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, b = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        db = np.sum(dout, axis=0)\n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NxqbT1aeTv-"
      },
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = softmax(x)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = self.out * dout\n",
        "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
        "        dx -= self.out * sumdx\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2YlK9SkeT2w"
      },
      "source": [
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.y = None  # softmax의 출력\n",
        "        self.t = None  # 정답 레이블\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "\n",
        "        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n",
        "        if self.t.size == self.y.size:\n",
        "            self.t = self.t.argmax(axis=1)\n",
        "\n",
        "        loss = cross_entropy_error(self.y, self.t)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        dx = self.y.copy()\n",
        "        dx[np.arange(batch_size), self.t] -= 1\n",
        "        dx *= dout\n",
        "        dx = dx / batch_size\n",
        "\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCJxR2-beT8g"
      },
      "source": [
        "class SGD:\n",
        "    '''\n",
        "    확률적 경사하강법(Stochastic Gradient Descent)\n",
        "    '''\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        for i in range(len(params)):\n",
        "            params[i] -= self.lr * grads[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMAOs9VyeZgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afaf3dbb-c446-452c-ba96-2b706aa0cd3e"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from dataset import spiral\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x, t = spiral.load_data()\n",
        "print('x', x.shape)\n",
        "print('t', t.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x (300, 2)\n",
            "t (300, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3YjghISeZmg"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "import numpy as np\n",
        "from common.layers import Affine, Sigmoid, SoftmaxWithLoss\n",
        "\n",
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        I, H, O = input_size, hidden_size, output_size\n",
        "\n",
        "        # 가중치와 편향 초기화\n",
        "        W1 = 0.01 * np.random.randn(I, H)\n",
        "        b1 = np.zeros(H)\n",
        "        W2 = 0.01 * np.random.randn(H, O)\n",
        "        b2 = np.zeros(O)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = [\n",
        "            Affine(W1, b1),\n",
        "            Sigmoid(),\n",
        "            Affine(W2, b2)\n",
        "        ]\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        score = self.predict(x)\n",
        "        loss = self.loss_layer.forward(score, t)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmAUoa8reZsV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33346398-3f58-4abe-c470-6af65fbb97e6"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "import numpy as np\n",
        "from common.optimizer import SGD\n",
        "from dataset import spiral\n",
        "import matplotlib.pyplot as plt\n",
        "from ch01.two_layer_net import TwoLayerNet\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "max_epoch = 300\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "learning_late = 1.0\n",
        "\n",
        "# 데이터 읽기, 모델과 옵티마이저 생성\n",
        "x, t = spiral.load_data()\n",
        "model = TwoLayerNet(input_size = 2, hidden_size = hidden_size, output_size = 3)\n",
        "optimizer = SGD(lr = learning_late)\n",
        "\n",
        "# 학습에 사용하는 변수\n",
        "data_size = len(x)\n",
        "max_iters = data_size // batch_size\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "loss_list = []\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    # 데이터 뒤섞기\n",
        "    idx = np.random.permutation(data_size)\n",
        "    x = x[idx]\n",
        "    t = t[idx]\n",
        "\n",
        "    for iters in range(max_iters):\n",
        "        batch_x = x[iters * batch_size : (iters + 1) * batch_size]\n",
        "        batch_t = t[iters * batch_size : (iters + 1) * batch_size]\n",
        "\n",
        "        # 기울기를 구해 매개변수 갱신\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "        optimizer.update(model.params, model.grads)\n",
        "\n",
        "        total_loss += loss\n",
        "        loss_count += 1\n",
        "\n",
        "        # 정기적으로 학습 경과 출력\n",
        "        if (iters + 1) % 10 == 0:\n",
        "            avg_loss = total_loss / loss_count\n",
        "            print('| 에폭 %d |  반복 %d / %d |  손실 %.2f' % (epoch + 1, iters + 1, max_iters, avg_loss))\n",
        "            loss_list.append(avg_loss)\n",
        "            total_loss, loss_count = 0, 0\n",
        "\n",
        "\n",
        "# 그래프 그리기\n",
        "\n",
        "# 학습 결과\n",
        "plt.plot(np.arange(len(loss_list)), loss_list, label='train')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "\n",
        "# 경계 영역\n",
        "h = 0.001\n",
        "x_min, x_max = x[:, 0].min() - .1, x[:, 0].max() + .1\n",
        "y_min, y_max = x[:, 1].min() - .1, x[:, 1].max() + .1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "X = np.c_[xx.ravel(), yy.ravel()]\n",
        "score = model.predict(X)\n",
        "predict_cls = np.argmax(score, axis=1)\n",
        "Z = predict_cls.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z)\n",
        "plt.axis('off')\n",
        "\n",
        "# 데이터점\n",
        "x, t = spiral.load_data()\n",
        "N = 100\n",
        "CLS_NUM = 3\n",
        "markers = ['o', 'x', '^']\n",
        "for i in range(CLS_NUM):\n",
        "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 10 / 10 |  손실 1.13\n",
            "| 에폭 2 |  반복 10 / 10 |  손실 1.13\n",
            "| 에폭 3 |  반복 10 / 10 |  손실 1.12\n",
            "| 에폭 4 |  반복 10 / 10 |  손실 1.12\n",
            "| 에폭 5 |  반복 10 / 10 |  손실 1.11\n",
            "| 에폭 6 |  반복 10 / 10 |  손실 1.14\n",
            "| 에폭 7 |  반복 10 / 10 |  손실 1.16\n",
            "| 에폭 8 |  반복 10 / 10 |  손실 1.11\n",
            "| 에폭 9 |  반복 10 / 10 |  손실 1.12\n",
            "| 에폭 10 |  반복 10 / 10 |  손실 1.13\n",
            "| 에폭 11 |  반복 10 / 10 |  손실 1.12\n",
            "| 에폭 12 |  반복 10 / 10 |  손실 1.11\n",
            "| 에폭 13 |  반복 10 / 10 |  손실 1.09\n",
            "| 에폭 14 |  반복 10 / 10 |  손실 1.08\n",
            "| 에폭 15 |  반복 10 / 10 |  손실 1.04\n",
            "| 에폭 16 |  반복 10 / 10 |  손실 1.03\n",
            "| 에폭 17 |  반복 10 / 10 |  손실 0.96\n",
            "| 에폭 18 |  반복 10 / 10 |  손실 0.92\n",
            "| 에폭 19 |  반복 10 / 10 |  손실 0.92\n",
            "| 에폭 20 |  반복 10 / 10 |  손실 0.87\n",
            "| 에폭 21 |  반복 10 / 10 |  손실 0.85\n",
            "| 에폭 22 |  반복 10 / 10 |  손실 0.82\n",
            "| 에폭 23 |  반복 10 / 10 |  손실 0.79\n",
            "| 에폭 24 |  반복 10 / 10 |  손실 0.78\n",
            "| 에폭 25 |  반복 10 / 10 |  손실 0.82\n",
            "| 에폭 26 |  반복 10 / 10 |  손실 0.78\n",
            "| 에폭 27 |  반복 10 / 10 |  손실 0.76\n",
            "| 에폭 28 |  반복 10 / 10 |  손실 0.76\n",
            "| 에폭 29 |  반복 10 / 10 |  손실 0.78\n",
            "| 에폭 30 |  반복 10 / 10 |  손실 0.75\n",
            "| 에폭 31 |  반복 10 / 10 |  손실 0.78\n",
            "| 에폭 32 |  반복 10 / 10 |  손실 0.77\n",
            "| 에폭 33 |  반복 10 / 10 |  손실 0.77\n",
            "| 에폭 34 |  반복 10 / 10 |  손실 0.78\n",
            "| 에폭 35 |  반복 10 / 10 |  손실 0.75\n",
            "| 에폭 36 |  반복 10 / 10 |  손실 0.74\n",
            "| 에폭 37 |  반복 10 / 10 |  손실 0.76\n",
            "| 에폭 38 |  반복 10 / 10 |  손실 0.76\n",
            "| 에폭 39 |  반복 10 / 10 |  손실 0.73\n",
            "| 에폭 40 |  반복 10 / 10 |  손실 0.75\n",
            "| 에폭 41 |  반복 10 / 10 |  손실 0.76\n",
            "| 에폭 42 |  반복 10 / 10 |  손실 0.76\n",
            "| 에폭 43 |  반복 10 / 10 |  손실 0.76\n",
            "| 에폭 44 |  반복 10 / 10 |  손실 0.74\n",
            "| 에폭 45 |  반복 10 / 10 |  손실 0.75\n",
            "| 에폭 46 |  반복 10 / 10 |  손실 0.73\n",
            "| 에폭 47 |  반복 10 / 10 |  손실 0.72\n",
            "| 에폭 48 |  반복 10 / 10 |  손실 0.73\n",
            "| 에폭 49 |  반복 10 / 10 |  손실 0.72\n",
            "| 에폭 50 |  반복 10 / 10 |  손실 0.72\n",
            "| 에폭 51 |  반복 10 / 10 |  손실 0.72\n",
            "| 에폭 52 |  반복 10 / 10 |  손실 0.72\n",
            "| 에폭 53 |  반복 10 / 10 |  손실 0.74\n",
            "| 에폭 54 |  반복 10 / 10 |  손실 0.74\n",
            "| 에폭 55 |  반복 10 / 10 |  손실 0.72\n",
            "| 에폭 56 |  반복 10 / 10 |  손실 0.72\n",
            "| 에폭 57 |  반복 10 / 10 |  손실 0.71\n",
            "| 에폭 58 |  반복 10 / 10 |  손실 0.70\n",
            "| 에폭 59 |  반복 10 / 10 |  손실 0.72\n",
            "| 에폭 60 |  반복 10 / 10 |  손실 0.70\n",
            "| 에폭 61 |  반복 10 / 10 |  손실 0.71\n",
            "| 에폭 62 |  반복 10 / 10 |  손실 0.72\n",
            "| 에폭 63 |  반복 10 / 10 |  손실 0.70\n",
            "| 에폭 64 |  반복 10 / 10 |  손실 0.71\n",
            "| 에폭 65 |  반복 10 / 10 |  손실 0.73\n",
            "| 에폭 66 |  반복 10 / 10 |  손실 0.70\n",
            "| 에폭 67 |  반복 10 / 10 |  손실 0.71\n",
            "| 에폭 68 |  반복 10 / 10 |  손실 0.69\n",
            "| 에폭 69 |  반복 10 / 10 |  손실 0.70\n",
            "| 에폭 70 |  반복 10 / 10 |  손실 0.71\n",
            "| 에폭 71 |  반복 10 / 10 |  손실 0.68\n",
            "| 에폭 72 |  반복 10 / 10 |  손실 0.69\n",
            "| 에폭 73 |  반복 10 / 10 |  손실 0.67\n",
            "| 에폭 74 |  반복 10 / 10 |  손실 0.68\n",
            "| 에폭 75 |  반복 10 / 10 |  손실 0.67\n",
            "| 에폭 76 |  반복 10 / 10 |  손실 0.66\n",
            "| 에폭 77 |  반복 10 / 10 |  손실 0.69\n",
            "| 에폭 78 |  반복 10 / 10 |  손실 0.64\n",
            "| 에폭 79 |  반복 10 / 10 |  손실 0.68\n",
            "| 에폭 80 |  반복 10 / 10 |  손실 0.64\n",
            "| 에폭 81 |  반복 10 / 10 |  손실 0.64\n",
            "| 에폭 82 |  반복 10 / 10 |  손실 0.66\n",
            "| 에폭 83 |  반복 10 / 10 |  손실 0.62\n",
            "| 에폭 84 |  반복 10 / 10 |  손실 0.62\n",
            "| 에폭 85 |  반복 10 / 10 |  손실 0.61\n",
            "| 에폭 86 |  반복 10 / 10 |  손실 0.60\n",
            "| 에폭 87 |  반복 10 / 10 |  손실 0.60\n",
            "| 에폭 88 |  반복 10 / 10 |  손실 0.61\n",
            "| 에폭 89 |  반복 10 / 10 |  손실 0.59\n",
            "| 에폭 90 |  반복 10 / 10 |  손실 0.58\n",
            "| 에폭 91 |  반복 10 / 10 |  손실 0.56\n",
            "| 에폭 92 |  반복 10 / 10 |  손실 0.56\n",
            "| 에폭 93 |  반복 10 / 10 |  손실 0.54\n",
            "| 에폭 94 |  반복 10 / 10 |  손실 0.53\n",
            "| 에폭 95 |  반복 10 / 10 |  손실 0.53\n",
            "| 에폭 96 |  반복 10 / 10 |  손실 0.52\n",
            "| 에폭 97 |  반복 10 / 10 |  손실 0.51\n",
            "| 에폭 98 |  반복 10 / 10 |  손실 0.50\n",
            "| 에폭 99 |  반복 10 / 10 |  손실 0.48\n",
            "| 에폭 100 |  반복 10 / 10 |  손실 0.48\n",
            "| 에폭 101 |  반복 10 / 10 |  손실 0.46\n",
            "| 에폭 102 |  반복 10 / 10 |  손실 0.45\n",
            "| 에폭 103 |  반복 10 / 10 |  손실 0.45\n",
            "| 에폭 104 |  반복 10 / 10 |  손실 0.44\n",
            "| 에폭 105 |  반복 10 / 10 |  손실 0.44\n",
            "| 에폭 106 |  반복 10 / 10 |  손실 0.41\n",
            "| 에폭 107 |  반복 10 / 10 |  손실 0.40\n",
            "| 에폭 108 |  반복 10 / 10 |  손실 0.41\n",
            "| 에폭 109 |  반복 10 / 10 |  손실 0.40\n",
            "| 에폭 110 |  반복 10 / 10 |  손실 0.40\n",
            "| 에폭 111 |  반복 10 / 10 |  손실 0.38\n",
            "| 에폭 112 |  반복 10 / 10 |  손실 0.38\n",
            "| 에폭 113 |  반복 10 / 10 |  손실 0.36\n",
            "| 에폭 114 |  반복 10 / 10 |  손실 0.37\n",
            "| 에폭 115 |  반복 10 / 10 |  손실 0.35\n",
            "| 에폭 116 |  반복 10 / 10 |  손실 0.34\n",
            "| 에폭 117 |  반복 10 / 10 |  손실 0.34\n",
            "| 에폭 118 |  반복 10 / 10 |  손실 0.34\n",
            "| 에폭 119 |  반복 10 / 10 |  손실 0.33\n",
            "| 에폭 120 |  반복 10 / 10 |  손실 0.34\n",
            "| 에폭 121 |  반복 10 / 10 |  손실 0.32\n",
            "| 에폭 122 |  반복 10 / 10 |  손실 0.32\n",
            "| 에폭 123 |  반복 10 / 10 |  손실 0.31\n",
            "| 에폭 124 |  반복 10 / 10 |  손실 0.31\n",
            "| 에폭 125 |  반복 10 / 10 |  손실 0.30\n",
            "| 에폭 126 |  반복 10 / 10 |  손실 0.30\n",
            "| 에폭 127 |  반복 10 / 10 |  손실 0.28\n",
            "| 에폭 128 |  반복 10 / 10 |  손실 0.28\n",
            "| 에폭 129 |  반복 10 / 10 |  손실 0.28\n",
            "| 에폭 130 |  반복 10 / 10 |  손실 0.28\n",
            "| 에폭 131 |  반복 10 / 10 |  손실 0.27\n",
            "| 에폭 132 |  반복 10 / 10 |  손실 0.27\n",
            "| 에폭 133 |  반복 10 / 10 |  손실 0.27\n",
            "| 에폭 134 |  반복 10 / 10 |  손실 0.27\n",
            "| 에폭 135 |  반복 10 / 10 |  손실 0.27\n",
            "| 에폭 136 |  반복 10 / 10 |  손실 0.26\n",
            "| 에폭 137 |  반복 10 / 10 |  손실 0.26\n",
            "| 에폭 138 |  반복 10 / 10 |  손실 0.26\n",
            "| 에폭 139 |  반복 10 / 10 |  손실 0.25\n",
            "| 에폭 140 |  반복 10 / 10 |  손실 0.24\n",
            "| 에폭 141 |  반복 10 / 10 |  손실 0.24\n",
            "| 에폭 142 |  반복 10 / 10 |  손실 0.25\n",
            "| 에폭 143 |  반복 10 / 10 |  손실 0.24\n",
            "| 에폭 144 |  반복 10 / 10 |  손실 0.24\n",
            "| 에폭 145 |  반복 10 / 10 |  손실 0.23\n",
            "| 에폭 146 |  반복 10 / 10 |  손실 0.24\n",
            "| 에폭 147 |  반복 10 / 10 |  손실 0.23\n",
            "| 에폭 148 |  반복 10 / 10 |  손실 0.23\n",
            "| 에폭 149 |  반복 10 / 10 |  손실 0.22\n",
            "| 에폭 150 |  반복 10 / 10 |  손실 0.22\n",
            "| 에폭 151 |  반복 10 / 10 |  손실 0.22\n",
            "| 에폭 152 |  반복 10 / 10 |  손실 0.22\n",
            "| 에폭 153 |  반복 10 / 10 |  손실 0.22\n",
            "| 에폭 154 |  반복 10 / 10 |  손실 0.22\n",
            "| 에폭 155 |  반복 10 / 10 |  손실 0.22\n",
            "| 에폭 156 |  반복 10 / 10 |  손실 0.21\n",
            "| 에폭 157 |  반복 10 / 10 |  손실 0.21\n",
            "| 에폭 158 |  반복 10 / 10 |  손실 0.20\n",
            "| 에폭 159 |  반복 10 / 10 |  손실 0.21\n",
            "| 에폭 160 |  반복 10 / 10 |  손실 0.20\n",
            "| 에폭 161 |  반복 10 / 10 |  손실 0.20\n",
            "| 에폭 162 |  반복 10 / 10 |  손실 0.20\n",
            "| 에폭 163 |  반복 10 / 10 |  손실 0.21\n",
            "| 에폭 164 |  반복 10 / 10 |  손실 0.20\n",
            "| 에폭 165 |  반복 10 / 10 |  손실 0.20\n",
            "| 에폭 166 |  반복 10 / 10 |  손실 0.19\n",
            "| 에폭 167 |  반복 10 / 10 |  손실 0.19\n",
            "| 에폭 168 |  반복 10 / 10 |  손실 0.19\n",
            "| 에폭 169 |  반복 10 / 10 |  손실 0.19\n",
            "| 에폭 170 |  반복 10 / 10 |  손실 0.19\n",
            "| 에폭 171 |  반복 10 / 10 |  손실 0.19\n",
            "| 에폭 172 |  반복 10 / 10 |  손실 0.18\n",
            "| 에폭 173 |  반복 10 / 10 |  손실 0.18\n",
            "| 에폭 174 |  반복 10 / 10 |  손실 0.18\n",
            "| 에폭 175 |  반복 10 / 10 |  손실 0.18\n",
            "| 에폭 176 |  반복 10 / 10 |  손실 0.18\n",
            "| 에폭 177 |  반복 10 / 10 |  손실 0.18\n",
            "| 에폭 178 |  반복 10 / 10 |  손실 0.18\n",
            "| 에폭 179 |  반복 10 / 10 |  손실 0.17\n",
            "| 에폭 180 |  반복 10 / 10 |  손실 0.17\n",
            "| 에폭 181 |  반복 10 / 10 |  손실 0.18\n",
            "| 에폭 182 |  반복 10 / 10 |  손실 0.17\n",
            "| 에폭 183 |  반복 10 / 10 |  손실 0.18\n",
            "| 에폭 184 |  반복 10 / 10 |  손실 0.17\n",
            "| 에폭 185 |  반복 10 / 10 |  손실 0.17\n",
            "| 에폭 186 |  반복 10 / 10 |  손실 0.18\n",
            "| 에폭 187 |  반복 10 / 10 |  손실 0.17\n",
            "| 에폭 188 |  반복 10 / 10 |  손실 0.17\n",
            "| 에폭 189 |  반복 10 / 10 |  손실 0.17\n",
            "| 에폭 190 |  반복 10 / 10 |  손실 0.17\n",
            "| 에폭 191 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 192 |  반복 10 / 10 |  손실 0.17\n",
            "| 에폭 193 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 194 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 195 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 196 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 197 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 198 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 199 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 200 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 201 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 202 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 203 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 204 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 205 |  반복 10 / 10 |  손실 0.16\n",
            "| 에폭 206 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 207 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 208 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 209 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 210 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 211 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 212 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 213 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 214 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 215 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 216 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 217 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 218 |  반복 10 / 10 |  손실 0.15\n",
            "| 에폭 219 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 220 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 221 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 222 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 223 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 224 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 225 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 226 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 227 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 228 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 229 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 230 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 231 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 232 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 233 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 234 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 235 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 236 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 237 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 238 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 239 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 240 |  반복 10 / 10 |  손실 0.14\n",
            "| 에폭 241 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 242 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 243 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 244 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 245 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 246 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 247 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 248 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 249 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 250 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 251 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 252 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 253 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 254 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 255 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 256 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 257 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 258 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 259 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 260 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 261 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 262 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 263 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 264 |  반복 10 / 10 |  손실 0.13\n",
            "| 에폭 265 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 266 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 267 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 268 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 269 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 270 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 271 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 272 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 273 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 274 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 275 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 276 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 277 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 278 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 279 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 280 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 281 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 282 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 283 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 284 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 285 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 286 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 287 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 288 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 289 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 290 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 291 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 292 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 293 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 294 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 295 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 296 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 297 |  반복 10 / 10 |  손실 0.12\n",
            "| 에폭 298 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 299 |  반복 10 / 10 |  손실 0.11\n",
            "| 에폭 300 |  반복 10 / 10 |  손실 0.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLixBbPqeZxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baffaab1-6c49-4a08-978d-02253a9a175b"
      },
      "source": [
        "import numpy as np\n",
        "np.random.permutation(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 5, 0, 9, 7, 6, 1, 2, 4, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGZmPx_CeZ2l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f13bb44a-a881-4775-960c-e3a557d18d7f"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append('os.chdir')\n",
        "from common.optimizer import SGD\n",
        "from common.trainer import Trainer\n",
        "from dataset import spiral\n",
        "from ch01.two_layer_net import TwoLayerNet\n",
        "\n",
        "max_epoch = 300\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "learning_rate = 1.0\n",
        "\n",
        "x, t = spiral.load_data()\n",
        "model = TwoLayerNet(input_size = 2, hidden_size = hidden_size, output_size = 3)\n",
        "optimize = SGD(lr = learning_rate)\n",
        "\n",
        "trainer = Trainer(model, optimizer)\n",
        "trainer.fit(x, t, max_epoch, batch_size, eval_interval = 10)\n",
        "trainer.plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 2 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 3 |  반복 1 / 10 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 4 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 5 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 6 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 7 |  반복 1 / 10 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 8 |  반복 1 / 10 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 9 |  반복 1 / 10 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 10 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 11 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 12 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 13 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 14 |  반복 1 / 10 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 15 |  반복 1 / 10 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 16 |  반복 1 / 10 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 17 |  반복 1 / 10 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 18 |  반복 1 / 10 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 19 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 20 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 21 |  반복 1 / 10 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 22 |  반복 1 / 10 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 23 |  반복 1 / 10 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 24 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 25 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 26 |  반복 1 / 10 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 27 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 28 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 29 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 30 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 31 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 32 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 33 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 34 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 35 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 36 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 37 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 38 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 39 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 40 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 41 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 42 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 43 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 44 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 45 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 46 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 47 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 48 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 49 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 50 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 51 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 52 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 53 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 54 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 55 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 56 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 57 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 58 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 59 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 60 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 61 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 62 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 63 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 64 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 65 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 66 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 67 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 68 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 69 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 70 |  반복 1 / 10 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 71 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 72 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 73 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 74 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 75 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 76 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 77 |  반복 1 / 10 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 78 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 79 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 80 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 81 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 82 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 83 |  반복 1 / 10 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 84 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 85 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 86 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 87 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 88 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 89 |  반복 1 / 10 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 90 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 91 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 92 |  반복 1 / 10 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 93 |  반복 1 / 10 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 94 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 95 |  반복 1 / 10 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 96 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 97 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 98 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 99 |  반복 1 / 10 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 100 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 101 |  반복 1 / 10 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 102 |  반복 1 / 10 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 103 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 104 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 105 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 106 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 107 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 108 |  반복 1 / 10 | 시간 0[s] | 손실 0.39\n",
            "| 에폭 109 |  반복 1 / 10 | 시간 0[s] | 손실 0.40\n",
            "| 에폭 110 |  반복 1 / 10 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 111 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 112 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 113 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 114 |  반복 1 / 10 | 시간 0[s] | 손실 0.37\n",
            "| 에폭 115 |  반복 1 / 10 | 시간 0[s] | 손실 0.36\n",
            "| 에폭 116 |  반복 1 / 10 | 시간 0[s] | 손실 0.34\n",
            "| 에폭 117 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 118 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
            "| 에폭 119 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 120 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
            "| 에폭 121 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
            "| 에폭 122 |  반복 1 / 10 | 시간 0[s] | 손실 0.32\n",
            "| 에폭 123 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 124 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 125 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 126 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
            "| 에폭 127 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
            "| 에폭 128 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
            "| 에폭 129 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
            "| 에폭 130 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
            "| 에폭 131 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 132 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
            "| 에폭 133 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
            "| 에폭 134 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
            "| 에폭 135 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 136 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
            "| 에폭 137 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
            "| 에폭 138 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 139 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 140 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 141 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 142 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 143 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 144 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 145 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
            "| 에폭 146 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
            "| 에폭 147 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
            "| 에폭 148 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 149 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 150 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 151 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 152 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 153 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 154 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 155 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 156 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 157 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 158 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 159 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 160 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 161 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 162 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 163 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 164 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 165 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 166 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 167 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 168 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 169 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 170 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 171 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 172 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 173 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 174 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 175 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 176 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 177 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 178 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 179 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 180 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 181 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 182 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 183 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 184 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 185 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 186 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 187 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 188 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 189 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 190 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 191 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 192 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 193 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 194 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 195 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 196 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 197 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 198 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 199 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 200 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 201 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 202 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 203 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 204 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 205 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 206 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 207 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 208 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 209 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 210 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 211 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 212 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 213 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 214 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 215 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 216 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 217 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 218 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 219 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 220 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 221 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 222 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 223 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 224 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 225 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 226 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 227 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 228 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 229 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 230 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 231 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 232 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 233 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 234 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 235 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 236 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 237 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 238 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 239 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 240 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 241 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 242 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 243 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 244 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 245 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 246 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 247 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 248 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 249 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 250 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 251 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 252 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 253 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 254 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 255 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 256 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 257 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 258 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 259 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 260 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 261 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 262 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 263 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 264 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 265 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 266 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 267 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 268 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 269 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 270 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 271 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 272 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 273 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 274 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 275 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 276 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 277 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 278 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 279 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 280 |  반복 1 / 10 | 시간 0[s] | 손실 0.10\n",
            "| 에폭 281 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 282 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 283 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 284 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 285 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 286 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 287 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 288 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 289 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 290 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 291 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 292 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 293 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 294 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 295 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 296 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 297 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 298 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 299 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 300 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCeVsXBCeZ7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56feeae4-337e-4f76-e499-18385dc094c2"
      },
      "source": [
        "import numpy as np\n",
        "a = np.random.randn(3)\n",
        "print(a.dtype)\n",
        "\n",
        "b = np.random.randn(3).astype(np.float32)\n",
        "print(b.dtype)\n",
        "\n",
        "c = np.random.randn(3).astype('f')\n",
        "print(c.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float64\n",
            "float32\n",
            "float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K0g0kWQeaAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56286f0-d39a-4b68-fca6-97abb717ed9c"
      },
      "source": [
        "# 말뭉치 전처리하기\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "text = text.lower()\n",
        "text = text.replace('.', ' .')\n",
        "\n",
        "print(text)\n",
        "\n",
        "words = text.split(' ')\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you say goodbye and i say hello .\n",
            "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcMsobhYxUEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a3e806-dbbb-4ee1-a94f-111dbe585662"
      },
      "source": [
        "word_to_id = {}\n",
        "id_to_word = {}\n",
        "\n",
        "for word in words:\n",
        "    if word not in word_to_id:\n",
        "        new_id = len(word_to_id)\n",
        "        word_to_id[word] = new_id\n",
        "        id_to_word[new_id] = word\n",
        "\n",
        "print(id_to_word)\n",
        "print(word_to_id)\n",
        "\n",
        "print(id_to_word[1])\n",
        "print(word_to_id['hello'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
            "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
            "say\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaxxB_MHyNUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b61121-6ceb-442f-a87e-2cf3029e8a20"
      },
      "source": [
        "import numpy as np\n",
        "#corpus = [word_to_id[w] for w in words]\n",
        "\n",
        "corpus = []\n",
        "for w in words:\n",
        "    corpus.append(word_to_id[w])\n",
        "corpus = np.array(corpus)\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZer_pXe0bX4"
      },
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' .')\n",
        "    words = text.split(' ')\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S82EGCF9185b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09edba69-0677-4a73-e7bb-96159a00cbbb"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append('os.chdir')\n",
        "import numpy as np\n",
        "from common.util import preprocess \n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "print(id_to_word)\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
            "[0 1 2 3 4 1 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR6sULt03KG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e39973f-988b-45fe-f19a-64c6b9e929e5"
      },
      "source": [
        "C = np.array([\n",
        "              [0, 1, 0, 0, 0, 0, 0],\n",
        "              [1, 0, 1, 0, 1, 1, 0],\n",
        "              [0, 1, 0, 1, 0, 0, 0],\n",
        "              [0, 0, 1, 0, 1, 0, 0],\n",
        "              [0, 1, 0, 1, 0, 0, 0],\n",
        "              [0, 1, 0, 0, 0, 0, 1],\n",
        "              [0, 0, 0, 0, 0, 1, 0],], dtype=np.int32)\n",
        "print(C.dtype)\n",
        "print(C[0], '\\n')\n",
        "\n",
        "print(C[4], '\\n')\n",
        "\n",
        "print(C[word_to_id['goodbye']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "int32\n",
            "[0 1 0 0 0 0 0] \n",
            "\n",
            "[0 1 0 1 0 0 0] \n",
            "\n",
            "[0 1 0 1 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j2QXz4G2jLo"
      },
      "source": [
        "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
        "    '''동시발생 행렬 생성\n",
        "\n",
        "    :param corpus: 말뭉치(단어 ID 목록)\n",
        "    :param vocab_size: 어휘 수\n",
        "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
        "    :return: 동시발생 행렬\n",
        "    '''\n",
        "    corpus_size = len(corpus)\n",
        "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        for i in range(1, window_size + 1):\n",
        "            left_idx = idx - i\n",
        "            right_idx = idx + i\n",
        "\n",
        "            if left_idx >= 0:\n",
        "                left_word_id = corpus[left_idx]\n",
        "                co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "            if right_idx < corpus_size:\n",
        "                right_word_id = corpus[right_idx]\n",
        "                co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "    return co_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNaBHcio7CwO"
      },
      "source": [
        "def cos_similarity(x, y):\n",
        "    nx = x / np.sqrt(np.sum(x**2))\n",
        "    ny = y / np.sqrt(np.sum(y**2))\n",
        "\n",
        "    return np.dot(nx, ny)\n",
        "\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
        "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
        "    \n",
        "    return np.dot(nx, ny)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EZllPw78j_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89361194-e759-49aa-d787-c3dc8df44b4a"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.util import preprocess, create_co_matrix, cos_similarity\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "\n",
        "c0 = C[word_to_id['you']]\n",
        "c1 = C[word_to_id['i']]\n",
        "\n",
        "print(cos_similarity(c0, c1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7071067691154799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k5ysyvd0Qul"
      },
      "source": [
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    '''유사 단어 검색\n",
        "\n",
        "    :param query: 쿼리(텍스트)\n",
        "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
        "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
        "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
        "    :param top: 상위 몇 개까지 출력할 지 지정\n",
        "    '''\n",
        "    if query not in word_to_id:\n",
        "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
        "        return\n",
        "\n",
        "    print('\\n[query] ' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "\n",
        "    # 코사인 유사도 계산\n",
        "    vocab_size = len(id_to_word)\n",
        "\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():    \n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq-E95P_29aW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22041c1-5a88-4063-8a71-673a407a3422"
      },
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "most_similar('you', word_to_id, id_to_word, C, top = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query] you\n",
            " goodbye: 0.7071067691154799\n",
            " i: 0.7071067691154799\n",
            " hello: 0.7071067691154799\n",
            " say: 0.0\n",
            " and: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT9kKETC5nsy"
      },
      "source": [
        "def ppmi(C, verbose = False, eps = 1e-8):\n",
        "    M = np.zeros_like(C ,dtype=np.float32)\n",
        "    N = np.sum(C)\n",
        "    S = np.sum(C, axis = 0)\n",
        "    total = C.shape[0] * C.shape[1]\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)\n",
        "            M[i, j] = max(0, pmi)\n",
        "\n",
        "            if verbose:\n",
        "                cnt += 1\n",
        "                if cnt & (total//100) == 0:\n",
        "                    print('%.1f%% 완료' % (100*cnt/total))\n",
        "\n",
        "    return M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_MCo1lq96cL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752b46a4-98f6-4f6d-fae0-7ebca8900b1b"
      },
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "W = ppmi(C)\n",
        "\n",
        "np.set_printoptions(precision=3)\n",
        "print('동시발생 행렬')\n",
        "print(C)\n",
        "\n",
        "print('-'*50)\n",
        "print('PPMI')\n",
        "print(W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "동시발생 행렬\n",
            "[[0 1 0 0 0 0 0]\n",
            " [1 0 1 0 1 1 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0]]\n",
            "--------------------------------------------------\n",
            "PPMI\n",
            "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.807 0.    0.    0.    0.    2.807]\n",
            " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1qjsfNmGqjk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "be977d09-e16d-4914-b2a6-956557758d79"
      },
      "source": [
        "# SVD에 의한 차원 감소\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "W = ppmi(C)\n",
        "\n",
        "# SVD\n",
        "U, S, V = np.linalg.svd(W)\n",
        "\n",
        "print(C[0])\n",
        "print(W[0])\n",
        "print(U[0])\n",
        " \n",
        "print(U[0, :2])\n",
        "\n",
        "# 그래프\n",
        "for word, word_id in word_to_id.items():\n",
        "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
        "\n",
        "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "print(U)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 0 0 0]\n",
            "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            "[ 3.409e-01  0.000e+00 -1.205e-01 -3.886e-16 -9.323e-01 -1.110e-16\n",
            " -2.426e-17]\n",
            "[0.341 0.   ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 3.409e-01  0.000e+00 -1.205e-01 -3.886e-16 -9.323e-01 -1.110e-16\n",
            "  -2.426e-17]\n",
            " [ 0.000e+00 -5.976e-01  0.000e+00  1.802e-01  0.000e+00 -7.812e-01\n",
            "   0.000e+00]\n",
            " [ 4.363e-01 -5.551e-17 -5.088e-01 -2.220e-16  2.253e-01 -1.388e-17\n",
            "  -7.071e-01]\n",
            " [ 1.110e-16 -4.978e-01  2.776e-17  6.804e-01 -1.110e-16  5.378e-01\n",
            "   7.467e-17]\n",
            " [ 4.363e-01 -3.124e-17 -5.088e-01 -1.600e-16  2.253e-01 -1.302e-17\n",
            "   7.071e-01]\n",
            " [ 7.092e-01 -3.124e-17  6.839e-01 -1.600e-16  1.710e-01 -1.302e-17\n",
            "   2.314e-17]\n",
            " [-1.665e-16 -6.285e-01 -4.163e-17 -7.103e-01  2.220e-16  3.169e-01\n",
            "  -9.614e-17]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPiGOAcVJZCu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "594d45e4-1938-4520-c8f9-78edce713082"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from dataset import ptb\n",
        "\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "\n",
        "print('말뭉치 크기:', len(corpus))\n",
        "print('corpus[:30]:', corpus[:30])\n",
        "print()\n",
        "print('id_to_word[0]:', id_to_word[0])\n",
        "print('id_to_word[1]:', id_to_word[1])\n",
        "print('id_to_word[2]:', id_to_word[2])\n",
        "print()\n",
        "print(\"word_to_id['car']:\", word_to_id['car'])\n",
        "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
        "print(\"word_to_id['lexus']:\", word_to_id['lexus'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "말뭉치 크기: 929589\n",
            "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29]\n",
            "\n",
            "id_to_word[0]: aer\n",
            "id_to_word[1]: banknote\n",
            "id_to_word[2]: berlitz\n",
            "\n",
            "word_to_id['car']: 3856\n",
            "word_to_id['happy']: 4428\n",
            "word_to_id['lexus']: 7426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdbnNY0-fGFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81ec028-d22b-4b79-9fae-b3665db987c3"
      },
      "source": [
        "# 3장\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "C = np.array([[1, 0, 0, 0, 0, 0, 0]])  # 입력\n",
        "W = np.random.randn(7, 3) # 가중치\n",
        "h = np.matmul(C, W)  # 중간 노드\n",
        "print(W)\n",
        "print(h)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.64   1.275  1.048]\n",
            " [ 1.53   1.726  1.081]\n",
            " [-0.558  1.453 -0.374]\n",
            " [ 2.322  0.099  0.055]\n",
            " [ 0.39  -0.163 -1.203]\n",
            " [ 1.807 -0.45  -1.981]\n",
            " [-1.382 -0.07   0.911]]\n",
            "[[0.64  1.275 1.048]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKEYWwCJhAfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64e1911-98d3-4d0b-b076-51603daf3a8e"
      },
      "source": [
        "# MatMul 계층으로 수행\n",
        "\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "import numpy as np\n",
        "from common.layers import MatMul\n",
        "\n",
        "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "W = np.random.randn(7, 3)\n",
        "layer = MatMul(W)\n",
        "h = layer.forward(c)\n",
        "print(h)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-2.184  1.728 -0.373]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rovLFLsRhkw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab316c2-2e0f-4c53-af64-392c47945a83"
      },
      "source": [
        "# CBOW 모델 추론 처리 구현\n",
        "\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "import numpy as np\n",
        "from common.layers import MatMul\n",
        "\n",
        "# 샘플 맥락 데이터\n",
        "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
        "\n",
        "# 가중치 초기화\n",
        "W_in = np.random.randn(7, 3)\n",
        "W_out = np.random.randn(3, 7)\n",
        "\n",
        "# 계층 생성\n",
        "in_layer0 = MatMul(W_in)\n",
        "in_layer1 = MatMul(W_in)\n",
        "out_layer = MatMul(W_out)\n",
        "\n",
        "# 순전파\n",
        "h0 = in_layer0.forward(c0)\n",
        "h1 = in_layer1.forward(c1)\n",
        "h = 0.5 * (h0 + h1)\n",
        "s = out_layer.forward(h)\n",
        "\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.529  1.091  0.695  1.36  -1.072 -0.802 -1.401]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do_6bgIilNKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1661634b-6650-41de-9f3c-0d9c12317dc8"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.util import preprocess\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "print(corpus)\n",
        "\n",
        "print(id_to_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6]\n",
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7HF1uljlbLz"
      },
      "source": [
        "def create_contexts_target(corpus, window_size = 1):\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus) - window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        contexts.append(cs)\n",
        "\n",
        "    return np.array(contexts), np.array(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DRLLF58mCjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a20f32-65a3-4416-8795-c0978f3a6588"
      },
      "source": [
        "# 함수 사용해보기\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size = 1)\n",
        "\n",
        "print(contexts)\n",
        "\n",
        "print(target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 2]\n",
            " [1 3]\n",
            " [2 4]\n",
            " [3 1]\n",
            " [4 5]\n",
            " [1 6]]\n",
            "[1 2 3 4 1 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsLCPAsxoF-1"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size = 1)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RifwxTazuwYc"
      },
      "source": [
        "# CBOW 모델 구현\n",
        "\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "import numpy as np\n",
        "from common.layers import MatMul, SoftmaxWithLoss\n",
        "\n",
        "\n",
        "class SimpleCBOW:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.in_layer0 = MatMul(W_in)\n",
        "        self.in_layer1 = MatMul(W_in)\n",
        "        self.out_layer = MatMul(W_out)\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
        "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
        "        h = (h0 + h1) * 0.5\n",
        "        score = self.out_layer.forward(h)\n",
        "        loss = self.loss_layer.forward(score, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        ds = self.loss_layer.backward(dout)\n",
        "        da = self.out_layer.backward(ds)\n",
        "        da *= 0.5\n",
        "        self.in_layer1.backward(da)\n",
        "        self.in_layer0.backward(da)\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKJOZFcWyrOC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2408af49-07a9-4a11-c21a-345b3e98e53a"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from ch03.simple_cbow import SimpleCBOW\n",
        "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
        "\n",
        "window_size = 1\n",
        "hidden_size = 5\n",
        "batch_size = 3\n",
        "max_epoch = 1000\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "\n",
        "model = SimpleCBOW(vocab_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()\n",
        "\n",
        "word_vecs = model.word_vecs\n",
        "for word_id, word in id_to_word.items():\n",
        "    print(word, word_vecs[word_id])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 2 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 3 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 4 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 5 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 6 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 7 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 8 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 9 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 10 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 11 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 12 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 13 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 14 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 15 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 16 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 17 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 18 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 19 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 20 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 21 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 22 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 23 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 24 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 25 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 26 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 27 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 28 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 29 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 30 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 31 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 32 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 33 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 34 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 35 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 36 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 37 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 38 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 39 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 40 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 41 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 42 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 43 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 44 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 45 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 46 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 47 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 48 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 49 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 50 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 51 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 52 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 53 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 54 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 55 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 56 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 57 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 58 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 59 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 60 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 61 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 62 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
            "| 에폭 63 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 64 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 65 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 66 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 67 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
            "| 에폭 68 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 69 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 70 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 71 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 72 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 73 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 74 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
            "| 에폭 75 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 76 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 77 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
            "| 에폭 78 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 79 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
            "| 에폭 80 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
            "| 에폭 81 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
            "| 에폭 82 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
            "| 에폭 83 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 84 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
            "| 에폭 85 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
            "| 에폭 86 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 87 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 88 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 89 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
            "| 에폭 90 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
            "| 에폭 91 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 92 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
            "| 에폭 93 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 94 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 95 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
            "| 에폭 96 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
            "| 에폭 97 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
            "| 에폭 98 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
            "| 에폭 99 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 100 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 101 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 102 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
            "| 에폭 103 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
            "| 에폭 104 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 105 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 106 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
            "| 에폭 107 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
            "| 에폭 108 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 109 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
            "| 에폭 110 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
            "| 에폭 111 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
            "| 에폭 112 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
            "| 에폭 113 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
            "| 에폭 114 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
            "| 에폭 115 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
            "| 에폭 116 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
            "| 에폭 117 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
            "| 에폭 118 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
            "| 에폭 119 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
            "| 에폭 120 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
            "| 에폭 121 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
            "| 에폭 122 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
            "| 에폭 123 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 124 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
            "| 에폭 125 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
            "| 에폭 126 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
            "| 에폭 127 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
            "| 에폭 128 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
            "| 에폭 129 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
            "| 에폭 130 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
            "| 에폭 131 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
            "| 에폭 132 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
            "| 에폭 133 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
            "| 에폭 134 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
            "| 에폭 135 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 136 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
            "| 에폭 137 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
            "| 에폭 138 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 139 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 140 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
            "| 에폭 141 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
            "| 에폭 142 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
            "| 에폭 143 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
            "| 에폭 144 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
            "| 에폭 145 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
            "| 에폭 146 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
            "| 에폭 147 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
            "| 에폭 148 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
            "| 에폭 149 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 150 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 151 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 152 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 153 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 154 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
            "| 에폭 155 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
            "| 에폭 156 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 157 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 158 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
            "| 에폭 159 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 160 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 161 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 162 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 163 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
            "| 에폭 164 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 165 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
            "| 에폭 166 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
            "| 에폭 167 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 168 |  반복 1 / 2 | 시간 0[s] | 손실 1.40\n",
            "| 에폭 169 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
            "| 에폭 170 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 171 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 172 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
            "| 에폭 173 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 174 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 175 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 176 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
            "| 에폭 177 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 178 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
            "| 에폭 179 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
            "| 에폭 180 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 181 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
            "| 에폭 182 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
            "| 에폭 183 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
            "| 에폭 184 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 185 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
            "| 에폭 186 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 187 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 188 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 189 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 190 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 191 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 192 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 193 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 194 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 195 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 196 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 197 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
            "| 에폭 198 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 199 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 200 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 201 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 202 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 203 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 204 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 205 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 206 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 207 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 208 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 209 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 210 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 211 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
            "| 에폭 212 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 213 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
            "| 에폭 214 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 215 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 216 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 217 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 218 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 219 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 220 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 221 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 222 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 223 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 224 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 225 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 226 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 227 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 228 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
            "| 에폭 229 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 230 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 231 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
            "| 에폭 232 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 233 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 234 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 235 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 236 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 237 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 238 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 239 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 240 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 241 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 242 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 243 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 244 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 245 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 246 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 247 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 248 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 249 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 250 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 251 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 252 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 253 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 254 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 255 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 256 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 257 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 258 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 259 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 260 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 261 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 262 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 263 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 264 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 265 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 266 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 267 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 268 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
            "| 에폭 269 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 270 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 271 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 272 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 273 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 274 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 275 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 276 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 277 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 278 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 279 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 280 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 281 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 282 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 283 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 284 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 285 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 286 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 287 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 288 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 289 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 290 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 291 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 292 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 293 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 294 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 295 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 296 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 297 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 298 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 299 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 300 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 301 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 302 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 303 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 304 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 305 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 306 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 307 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 308 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 309 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 310 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 311 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 312 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 313 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 314 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 315 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 316 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 317 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 318 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 319 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 320 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 321 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 322 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 323 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 324 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 325 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 326 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 327 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 328 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 329 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 330 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 331 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 332 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 333 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 334 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 335 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 336 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 337 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 338 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 339 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 340 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 341 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 342 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 343 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 344 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 345 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 346 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 347 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 348 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 349 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 350 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 351 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 352 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 353 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 354 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 355 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 356 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 357 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 358 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 359 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 360 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 361 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 362 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 363 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 364 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 365 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 366 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 367 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 368 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 369 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 370 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 371 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 372 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 373 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 374 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 375 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 376 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 377 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 378 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 379 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 380 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 381 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 382 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 383 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 384 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 385 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 386 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 387 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 388 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 389 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 390 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 391 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 392 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 393 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 394 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 395 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 396 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 397 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 398 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 399 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 400 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 401 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 402 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 403 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 404 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 405 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 406 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 407 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 408 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 409 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 410 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 411 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 412 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 413 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 414 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 415 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 416 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 417 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 418 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 419 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 420 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 421 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 422 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 423 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 424 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 425 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 426 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 427 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 428 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 429 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 430 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 431 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 432 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 433 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 434 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 435 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 436 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 437 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 438 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 439 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 440 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 441 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 442 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 443 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 444 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 445 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 446 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 447 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 448 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 449 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 450 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 451 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 452 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 453 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 454 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 455 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 456 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 457 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 458 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 459 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 460 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 461 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 462 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 463 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 464 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 465 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 466 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 467 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 468 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 469 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 470 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 471 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 472 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 473 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 474 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 475 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 476 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 477 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 478 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 479 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 480 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 481 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 482 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 483 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 484 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 485 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 486 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 487 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 488 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 489 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 490 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 491 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 492 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 493 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 494 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 495 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 496 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 497 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 498 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 499 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 500 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 501 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 502 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 503 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 504 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 505 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 506 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 507 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 508 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 509 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 510 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 511 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 512 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 513 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 514 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 515 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 516 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 517 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 518 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 519 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 520 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 521 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 522 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 523 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 524 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 525 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 526 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 527 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 528 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 529 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 530 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 531 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 532 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 533 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 534 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 535 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 536 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 537 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 538 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 539 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 540 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 541 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 542 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 543 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 544 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 545 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 546 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 547 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 548 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 549 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 550 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 551 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 552 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 553 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 554 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 555 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 556 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 557 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 558 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 559 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 560 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 561 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 562 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 563 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 564 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 565 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 566 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 567 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 568 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 569 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 570 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 571 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 572 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 573 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 574 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 575 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 576 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 577 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 578 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 579 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 580 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 581 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 582 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 583 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 584 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 585 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 586 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 587 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 588 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 589 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 590 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 591 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 592 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 593 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 594 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 595 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 596 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 597 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 598 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 599 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 600 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 601 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 602 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 603 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 604 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 605 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 606 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 607 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 608 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 609 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 610 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 611 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 612 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 613 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 614 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 615 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 616 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 617 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 618 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 619 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 620 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 621 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 622 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 623 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 624 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 625 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 626 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 627 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 628 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 629 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 630 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 631 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 632 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 633 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 634 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 635 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 636 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 637 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 638 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 639 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 640 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 641 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 642 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 643 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 644 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 645 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 646 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 647 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 648 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 649 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 650 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 651 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 652 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 653 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 654 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 655 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 656 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 657 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 658 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 659 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 660 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 661 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 662 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 663 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 664 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 665 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 666 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 667 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 668 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 669 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 670 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 671 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 672 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 673 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 674 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 675 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 676 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 677 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 678 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 679 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 680 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 681 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 682 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 683 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 684 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 685 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 686 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 687 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 688 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 689 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 690 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 691 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 692 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 693 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 694 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 695 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 696 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 697 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 698 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 699 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 700 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 701 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 702 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 703 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 704 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 705 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 706 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 707 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 708 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 709 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 710 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 711 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 712 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 713 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 714 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 715 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 716 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 717 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 718 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 719 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 720 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 721 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 722 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 723 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 724 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 725 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 726 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 727 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 728 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 729 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 730 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 731 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 732 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 733 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 734 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 735 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 736 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 737 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 738 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 739 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 740 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 741 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 742 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 743 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 744 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 745 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 746 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 747 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 748 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 749 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 750 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 751 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 752 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 753 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 754 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 755 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 756 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 757 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 758 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 759 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 760 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 761 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 762 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 763 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 764 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 765 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 766 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 767 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 768 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 769 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 770 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 771 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 772 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 773 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 774 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 775 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 776 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
            "| 에폭 777 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 778 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 779 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 780 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 781 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 782 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 783 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 784 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 785 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 786 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 787 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 788 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 789 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 790 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 791 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 792 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 793 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 794 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 795 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 796 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 797 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 798 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 799 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 800 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 801 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 802 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 803 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 804 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 805 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 806 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 807 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 808 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 809 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 810 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 811 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 812 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 813 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 814 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 815 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 816 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 817 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 818 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 819 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 820 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 821 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 822 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 823 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 824 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 825 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 826 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 827 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 828 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 829 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 830 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 831 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 832 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 833 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 834 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 835 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 836 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 837 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 838 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 839 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 840 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 841 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 842 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 843 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 844 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 845 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 846 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 847 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 848 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 849 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 850 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 851 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 852 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 853 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 854 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 855 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 856 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 857 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 858 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 859 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 860 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 861 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 862 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 863 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 864 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 865 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 866 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 867 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 868 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 869 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 870 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 871 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 872 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 873 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 874 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 875 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 876 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 877 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 878 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 879 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 880 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 881 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 882 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
            "| 에폭 883 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 884 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 885 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 886 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 887 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 888 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 889 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 890 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 891 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 892 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 893 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 894 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 895 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 896 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 897 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 898 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 899 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 900 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 901 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 902 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 903 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 904 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 905 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
            "| 에폭 906 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 907 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 908 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 909 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 910 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 911 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 912 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 913 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 914 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 915 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 916 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 917 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 918 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 919 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 920 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 921 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 922 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 923 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 924 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 925 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 926 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 927 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 928 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 929 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 930 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 931 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 932 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 933 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 934 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 935 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 936 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 937 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 938 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 939 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 940 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 941 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 942 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 943 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 944 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 945 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 946 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 947 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 948 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 949 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 950 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 951 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 952 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 953 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 954 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 955 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 956 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 957 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 958 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 959 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 960 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 961 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 962 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 963 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 964 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 965 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 966 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 967 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 968 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 969 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 970 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 971 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 972 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 973 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 974 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 975 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 976 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 977 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 978 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 979 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 980 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 981 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 982 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 983 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 984 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 985 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 986 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 987 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 988 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 989 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 990 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 991 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 992 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 993 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 994 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 995 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 996 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 997 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 998 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 999 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 1000 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "you [-1.089  1.049 -1.378 -1.077 -1.042]\n",
            "say [ 1.149 -1.152 -0.206  1.148  1.2  ]\n",
            "goodbye [-0.841  0.882 -0.54  -0.794 -0.82 ]\n",
            "and [ 0.846 -0.85  -1.911  0.778  0.753]\n",
            "i [-0.841  0.886 -0.522 -0.788 -0.821]\n",
            "hello [-1.081  1.027 -1.407 -1.072 -1.04 ]\n",
            ". [ 1.16  -1.151  1.651  1.17   1.218]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5ON4L9k10Yq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbe4be3-6cc3-4d6e-9a6a-7df7c75a1346"
      },
      "source": [
        "import numpy as np\n",
        "W = np.arange(21).reshape(7, 3)\n",
        "print(W)\n",
        "print()\n",
        "print(W[2])\n",
        "print()\n",
        "print(W[5])\n",
        "print()\n",
        "\n",
        "idx = np.array([1, 0, 3, 0])\n",
        "print(W[idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2]\n",
            " [ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]\n",
            " [12 13 14]\n",
            " [15 16 17]\n",
            " [18 19 20]]\n",
            "\n",
            "[6 7 8]\n",
            "\n",
            "[15 16 17]\n",
            "\n",
            "[[ 3  4  5]\n",
            " [ 0  1  2]\n",
            " [ 9 10 11]\n",
            " [ 0  1  2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fooMsmmY2NgI"
      },
      "source": [
        "class Embedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.idx = None\n",
        "\n",
        "    def forward(self, idx):\n",
        "        W, = self.params\n",
        "        self.idx = idx\n",
        "        out = W[idx]\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dw, = self.grads\n",
        "        dw[...] = 0\n",
        "\n",
        "        for i, word_id in enumerate(self.idx):\n",
        "            dw[word_id] += dout[i]\n",
        "        # 혹은 np.add.at(dW, self.idx, dout)\n",
        "\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3XsCOs5GkSj"
      },
      "source": [
        "class EmbeddingDot:\n",
        "    def __init__(self, W):\n",
        "        self.embed = Embedding(W)\n",
        "        self.params = self.embed.params\n",
        "        self.grads = self.embed.grads\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, h, idx):\n",
        "        target_W = self.embed.forward(idx)\n",
        "        out = np.sum(target_W * h, axis = 1)\n",
        "\n",
        "        self.cache = (h, target_W)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        h, target_W = self.cache\n",
        "        dout = dout.reshape(dout.shape[0], 1)\n",
        "\n",
        "        dtarget_W = dout * h\n",
        "        self.embed.backward(dtarget_W)\n",
        "        dh = dout * target_W\n",
        "        return dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfL3ICR4jG4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811e24ad-d1e9-411b-ec4f-b298cac7852a"
      },
      "source": [
        "import numpy as np\n",
        "# 0에서 9까지의 숫자 중 하나를 무작위로 샘플링\n",
        "print(np.random.choice(10))\n",
        "print(np.random.choice(10))\n",
        "\n",
        "# words에서 하나만 무작위로 샘플링\n",
        "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
        "print(np.random.choice(words))\n",
        "\n",
        "# 5개만 무작위로 샘플링(중복 있음)\n",
        "print(np.random.choice(words, size=5))\n",
        "\n",
        "# 5개만 무작위로 샘플링(중복 없음)\n",
        "print(np.random.choice(words, size=5, replace=False))\n",
        "\n",
        "# 확률분포에 따라 샘플링\n",
        "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
        "print(np.random.choice(words, p=p))\n",
        "\n",
        "p = [0.7, 0.29, 0.01]\n",
        "new_p = np.power(p, 0.75)\n",
        "new_p /= np.sum(new_p)\n",
        "print(new_p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "4\n",
            "I\n",
            "['I' 'I' 'I' 'goodbye' 'hello']\n",
            "['I' 'hello' 'you' '.' 'say']\n",
            "you\n",
            "[0.642 0.332 0.027]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shZvJNuklS5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63143a36-ecf3-4a76-d86b-001342edf3e5"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from ch04.negative_sampling_layer import UnigramSampler\n",
        "\n",
        "corpus = np.array([0, 1, 2, 3, 4, 1, 2 ,3])\n",
        "power = 0.75\n",
        "sample_size = 2\n",
        "\n",
        "sampler = UnigramSampler(corpus, power, sample_size)\n",
        "target = np.array([1, 3, 0])\n",
        "negative_sample = sampler.get_negative_sample(target)\n",
        "print(negative_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 2]\n",
            " [2 1]\n",
            " [2 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UES1XCLpnhV-"
      },
      "source": [
        "class NegativeSamplingLoss:\n",
        "    def __init__(self, W, corpus, power = 0.75, sample_size = 5):\n",
        "        self.sample_size = sample_size\n",
        "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "        self.params, self.grads = [], []\n",
        "\n",
        "        for layer in self.embed_dot_layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        def forward(self, h, target):\n",
        "            batch_size = target.shape[0]\n",
        "            negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "            # 긍정적 예 순전파\n",
        "            score = self.embed_dot_layers[0].forward(h, target)\n",
        "            correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "            loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "            # 부정적 예 순전파\n",
        "            negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "            for i in range(self.sample_size):\n",
        "                negative_target = negative_sample[:, i]\n",
        "                score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "                loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "\n",
        "            return loss\n",
        "\n",
        "        def backward(self, dout=1):\n",
        "            dh = 0\n",
        "            for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "                dscore = l0.backward(dout)\n",
        "                dh += l1.backward(dscore)\n",
        "\n",
        "            return dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyz7hjpYxDZe"
      },
      "source": [
        "# CBOW 모델 구현\n",
        "\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "import numpy as np\n",
        "from common.layers import Embedding\n",
        "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
        "\n",
        "class CBOW:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.in_layers = []\n",
        "        for i in range(2 * windos_size):\n",
        "            layer = Embedding(W_in) # Embedding 계층 사용\n",
        "            self.in_layers.append(layer)\n",
        "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "        # 모든 가중치와 기울기를 배열에 모은다.\n",
        "        layers = self.in_layers + [self.ns_loss]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h = 0\n",
        "        for i, layer in enumerate(self.in_layers):\n",
        "            h += layer.forward(contexts[:, i])\n",
        "        h *= 1 / len(self.in_layers)\n",
        "        loss = self.ns_loss.forward(h, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.ns_loss.backward(dout)\n",
        "        dout *= 1 / len(self.in_layers)\n",
        "        for layer in self.in_layers:\n",
        "            layer.backward(dout)\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOkmQAMO2jQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7130708c-5e05-4507-c604-e66244bdd6ff"
      },
      "source": [
        "# CBOW 계층 구현\n",
        "import sys,os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.util import most_similar, analogy\n",
        "import pickle\n",
        "\n",
        "\n",
        "pkl_file = '/content/drive/My Drive/deep-learning-from-scratch-2-master/ch04/cbow_params.pkl'\n",
        "\n",
        "with open(pkl_file, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "    word_vecs = params['word_vecs']\n",
        "    word_to_id = params['word_to_id']\n",
        "    id_to_word = params['id_to_word']\n",
        "\n",
        "# 가장 비슷한(most similar) 단어 뽑기\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n",
        "\n",
        "# 유추(analogy) 작업\n",
        "print('-'*50)\n",
        "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query] you\n",
            " we: 0.6103515625\n",
            " someone: 0.59130859375\n",
            " i: 0.55419921875\n",
            " something: 0.48974609375\n",
            " anyone: 0.47314453125\n",
            "\n",
            "[query] year\n",
            " month: 0.71875\n",
            " week: 0.65234375\n",
            " spring: 0.62744140625\n",
            " summer: 0.6259765625\n",
            " decade: 0.603515625\n",
            "\n",
            "[query] car\n",
            " luxury: 0.497314453125\n",
            " arabia: 0.47802734375\n",
            " auto: 0.47119140625\n",
            " disk-drive: 0.450927734375\n",
            " travel: 0.4091796875\n",
            "\n",
            "[query] toyota\n",
            " ford: 0.55078125\n",
            " instrumentation: 0.509765625\n",
            " mazda: 0.49365234375\n",
            " bethlehem: 0.47509765625\n",
            " nissan: 0.474853515625\n",
            "--------------------------------------------------\n",
            "\n",
            "[analogy] king:man = queen:?\n",
            " woman: 5.16015625\n",
            " veto: 4.9296875\n",
            " ounce: 4.69140625\n",
            " earthquake: 4.6328125\n",
            " successor: 4.609375\n",
            "\n",
            "[analogy] take:took = go:?\n",
            " went: 4.55078125\n",
            " points: 4.25\n",
            " began: 4.09375\n",
            " comes: 3.98046875\n",
            " oct.: 3.90625\n",
            "\n",
            "[analogy] car:cars = child:?\n",
            " children: 5.21875\n",
            " average: 4.7265625\n",
            " yield: 4.20703125\n",
            " cattle: 4.1875\n",
            " priced: 4.1796875\n",
            "\n",
            "[analogy] good:better = bad:?\n",
            " more: 6.6484375\n",
            " less: 6.0625\n",
            " rather: 5.21875\n",
            " slower: 4.734375\n",
            " greater: 4.671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWw1pkWk28pV"
      },
      "source": [
        "# RNN 구현\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b\n",
        "        h_next = np.math(t)\n",
        "\n",
        "        self.cache = (x, h_prev, h_next)\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, h_next = self.cache\n",
        "\n",
        "        dt = dh_next * (1 - h_next ** 2)\n",
        "        db = np.sum(dt, axis = 0)\n",
        "        dWh = np.matmul(h_prev.T, dt)\n",
        "        dh_prev = np.matmul(dt, Wh.T)\n",
        "        dWx = np.matmul(x.T, dt)\n",
        "        dx = np.matmul(dt, Wx.T)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        return dx, dh_prev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Man3IX7vHgTu"
      },
      "source": [
        "#Time RNN 계층 구현 => T개의 RNN 계층으로 구성\n",
        "\n",
        "class TimeRNN:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = RNN(*self.params)\n",
        "            self.h = layer.forward(xs[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh = 0\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "            dxs[:, t, :] = dx\n",
        "\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzdEGEmuO-Ix"
      },
      "source": [
        "# RNNLM 구현\n",
        "\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "import numpy as np\n",
        "from common.time_layers import *\n",
        "\n",
        "class SimpleRnnlm:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # 가중치 초기화\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
        "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
        "        rnn_b = np.zeros(H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b)\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.rnn_layer = self.layers[1]\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        loss = self.loss_layer.forward(xs, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.rnn_layer.reset_state()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtjSQivFPlzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bdd931-0c36-46a3-bc68-5243f478e973"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from common.optimizer import SGD\n",
        "from dataset import ptb\n",
        "from ch05.simple_rnnlm import SimpleRnnlm\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 10\n",
        "wordvec_size = 100\n",
        "hidden_size = 100 # RNN의 은닉 상태 벡터의 원소 수\n",
        "time_size = 5 # Truncated BPTT가 한 번에 펼치는 시간 크기\n",
        "lr = 0.1\n",
        "max_epoch = 100\n",
        "\n",
        "# 학습 데이터 읽기(전체 중 1000개만)\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "corpus_size = 1000\n",
        "corpus = corpus[:corpus_size]\n",
        "vocab_size = int(max(corpus) + 1)\n",
        "\n",
        "xs = corpus[:1] # 입력\n",
        "ts = corpus[1:] # 출력(정답 레이블)\n",
        "data_size = len(xs)\n",
        "print('말뭉치 크기: %d, 어휘 수: %d' % (corpus_size, vocab_size))\n",
        "\n",
        "# 학습 시 사용하는 변수\n",
        "max_iters = data_size // (batch_size * time_size)\n",
        "time_idx = 0\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "ppl_list = []\n",
        "\n",
        "# 모델 생성\n",
        "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = SGD(lr)\n",
        "\n",
        "# 각 미니배치에서 샘플을 읽기 시작 위치를 계산\n",
        "jump = (corpus_size - 1) // batch_size\n",
        "offsets = [i * jump for i in range(batch_size)]\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    for iter in range(max_iters):\n",
        "        # 미니배치 획득\n",
        "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
        "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
        "        for t in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
        "                batch_t[i, t] = xs[(offset + time_dix) % data_size]\n",
        "            time_idx += 1\n",
        "        \n",
        "        # 기울기를 구하여 매개변수 갱신\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "        optimizer.update(model.params, model.grads)\n",
        "        total_loss += loss\n",
        "        loss_count += 1\n",
        "\n",
        "    # 에폭마다 퍼플렉서티 평가\n",
        "    ppl = np.exp(total_loss / (loss_count + 1e-8))\n",
        "    print('| 에폭 %d | 퍼플렉서티 %.2f' % (epoch+1, ppl))\n",
        "    ppl_list.append(float(ppl))\n",
        "    total_loss, loss_count = 0, 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "말뭉치 크기: 1000, 어휘 수: 418\n",
            "| 에폭 1 | 퍼플렉서티 1.00\n",
            "| 에폭 2 | 퍼플렉서티 1.00\n",
            "| 에폭 3 | 퍼플렉서티 1.00\n",
            "| 에폭 4 | 퍼플렉서티 1.00\n",
            "| 에폭 5 | 퍼플렉서티 1.00\n",
            "| 에폭 6 | 퍼플렉서티 1.00\n",
            "| 에폭 7 | 퍼플렉서티 1.00\n",
            "| 에폭 8 | 퍼플렉서티 1.00\n",
            "| 에폭 9 | 퍼플렉서티 1.00\n",
            "| 에폭 10 | 퍼플렉서티 1.00\n",
            "| 에폭 11 | 퍼플렉서티 1.00\n",
            "| 에폭 12 | 퍼플렉서티 1.00\n",
            "| 에폭 13 | 퍼플렉서티 1.00\n",
            "| 에폭 14 | 퍼플렉서티 1.00\n",
            "| 에폭 15 | 퍼플렉서티 1.00\n",
            "| 에폭 16 | 퍼플렉서티 1.00\n",
            "| 에폭 17 | 퍼플렉서티 1.00\n",
            "| 에폭 18 | 퍼플렉서티 1.00\n",
            "| 에폭 19 | 퍼플렉서티 1.00\n",
            "| 에폭 20 | 퍼플렉서티 1.00\n",
            "| 에폭 21 | 퍼플렉서티 1.00\n",
            "| 에폭 22 | 퍼플렉서티 1.00\n",
            "| 에폭 23 | 퍼플렉서티 1.00\n",
            "| 에폭 24 | 퍼플렉서티 1.00\n",
            "| 에폭 25 | 퍼플렉서티 1.00\n",
            "| 에폭 26 | 퍼플렉서티 1.00\n",
            "| 에폭 27 | 퍼플렉서티 1.00\n",
            "| 에폭 28 | 퍼플렉서티 1.00\n",
            "| 에폭 29 | 퍼플렉서티 1.00\n",
            "| 에폭 30 | 퍼플렉서티 1.00\n",
            "| 에폭 31 | 퍼플렉서티 1.00\n",
            "| 에폭 32 | 퍼플렉서티 1.00\n",
            "| 에폭 33 | 퍼플렉서티 1.00\n",
            "| 에폭 34 | 퍼플렉서티 1.00\n",
            "| 에폭 35 | 퍼플렉서티 1.00\n",
            "| 에폭 36 | 퍼플렉서티 1.00\n",
            "| 에폭 37 | 퍼플렉서티 1.00\n",
            "| 에폭 38 | 퍼플렉서티 1.00\n",
            "| 에폭 39 | 퍼플렉서티 1.00\n",
            "| 에폭 40 | 퍼플렉서티 1.00\n",
            "| 에폭 41 | 퍼플렉서티 1.00\n",
            "| 에폭 42 | 퍼플렉서티 1.00\n",
            "| 에폭 43 | 퍼플렉서티 1.00\n",
            "| 에폭 44 | 퍼플렉서티 1.00\n",
            "| 에폭 45 | 퍼플렉서티 1.00\n",
            "| 에폭 46 | 퍼플렉서티 1.00\n",
            "| 에폭 47 | 퍼플렉서티 1.00\n",
            "| 에폭 48 | 퍼플렉서티 1.00\n",
            "| 에폭 49 | 퍼플렉서티 1.00\n",
            "| 에폭 50 | 퍼플렉서티 1.00\n",
            "| 에폭 51 | 퍼플렉서티 1.00\n",
            "| 에폭 52 | 퍼플렉서티 1.00\n",
            "| 에폭 53 | 퍼플렉서티 1.00\n",
            "| 에폭 54 | 퍼플렉서티 1.00\n",
            "| 에폭 55 | 퍼플렉서티 1.00\n",
            "| 에폭 56 | 퍼플렉서티 1.00\n",
            "| 에폭 57 | 퍼플렉서티 1.00\n",
            "| 에폭 58 | 퍼플렉서티 1.00\n",
            "| 에폭 59 | 퍼플렉서티 1.00\n",
            "| 에폭 60 | 퍼플렉서티 1.00\n",
            "| 에폭 61 | 퍼플렉서티 1.00\n",
            "| 에폭 62 | 퍼플렉서티 1.00\n",
            "| 에폭 63 | 퍼플렉서티 1.00\n",
            "| 에폭 64 | 퍼플렉서티 1.00\n",
            "| 에폭 65 | 퍼플렉서티 1.00\n",
            "| 에폭 66 | 퍼플렉서티 1.00\n",
            "| 에폭 67 | 퍼플렉서티 1.00\n",
            "| 에폭 68 | 퍼플렉서티 1.00\n",
            "| 에폭 69 | 퍼플렉서티 1.00\n",
            "| 에폭 70 | 퍼플렉서티 1.00\n",
            "| 에폭 71 | 퍼플렉서티 1.00\n",
            "| 에폭 72 | 퍼플렉서티 1.00\n",
            "| 에폭 73 | 퍼플렉서티 1.00\n",
            "| 에폭 74 | 퍼플렉서티 1.00\n",
            "| 에폭 75 | 퍼플렉서티 1.00\n",
            "| 에폭 76 | 퍼플렉서티 1.00\n",
            "| 에폭 77 | 퍼플렉서티 1.00\n",
            "| 에폭 78 | 퍼플렉서티 1.00\n",
            "| 에폭 79 | 퍼플렉서티 1.00\n",
            "| 에폭 80 | 퍼플렉서티 1.00\n",
            "| 에폭 81 | 퍼플렉서티 1.00\n",
            "| 에폭 82 | 퍼플렉서티 1.00\n",
            "| 에폭 83 | 퍼플렉서티 1.00\n",
            "| 에폭 84 | 퍼플렉서티 1.00\n",
            "| 에폭 85 | 퍼플렉서티 1.00\n",
            "| 에폭 86 | 퍼플렉서티 1.00\n",
            "| 에폭 87 | 퍼플렉서티 1.00\n",
            "| 에폭 88 | 퍼플렉서티 1.00\n",
            "| 에폭 89 | 퍼플렉서티 1.00\n",
            "| 에폭 90 | 퍼플렉서티 1.00\n",
            "| 에폭 91 | 퍼플렉서티 1.00\n",
            "| 에폭 92 | 퍼플렉서티 1.00\n",
            "| 에폭 93 | 퍼플렉서티 1.00\n",
            "| 에폭 94 | 퍼플렉서티 1.00\n",
            "| 에폭 95 | 퍼플렉서티 1.00\n",
            "| 에폭 96 | 퍼플렉서티 1.00\n",
            "| 에폭 97 | 퍼플렉서티 1.00\n",
            "| 에폭 98 | 퍼플렉서티 1.00\n",
            "| 에폭 99 | 퍼플렉서티 1.00\n",
            "| 에폭 100 | 퍼플렉서티 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zkgYKi5myvF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "7fc02aab-0c60-407e-9733-5981adf6b634"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 2 # 미니배치 크기\n",
        "H = 3 # 은닉 상태 벡터의 차원 수\n",
        "T = 20 # 시계열 데이터의 길이\n",
        "\n",
        "dh = np.ones((N, H))\n",
        "np.random.seed(3) # 재현할 수 있도록 난수의 시드 고정\n",
        "Wh = np.random.randn(H, H)\n",
        "\n",
        "norm_list = []\n",
        "for t in range(T):\n",
        "    dh = np.matmul(dh, Wh.T)\n",
        "    norm = np.sqrt(np.sum(dh**2)) / N\n",
        "    norm_list.append(norm)\n",
        "\n",
        "print(norm_list)\n",
        "\n",
        "# 그래프 그리기\n",
        "plt.plot(np.arange(len(norm_list)), norm_list)\n",
        "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
        "plt.xlabel('time step')\n",
        "plt.ylabel('norm')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.4684068094579303, 3.335704974161037, 4.783279375373183, 6.2795873320876145, 8.080776465019055, 10.25116303229294, 12.9360635066099, 16.276861327786712, 20.454829618345983, 25.688972842084684, 32.25315718048336, 40.48895641683869, 50.824407307019094, 63.79612654485427, 80.07737014308985, 100.51298922051251, 126.16331847536827, 158.3592064825883, 198.77107967611957, 249.495615421267]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHLNx3DGuU8k"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "dW1 = np.random.rand(3, 3) * 10\n",
        "dW2 = np.random.rand(3, 3) * 10\n",
        "grads = [dW1, dW2]\n",
        "max_norm = 5.0\n",
        "\n",
        "def clip_grads(grads, max_norm):\n",
        "    total_norm = 0\n",
        "    for grad in grads:\n",
        "        total_norm += np.sum(gradd ** 2)\n",
        "    total_norm = np.sqrt(total_norm)\n",
        "\n",
        "    rate = max_norm / (total_norm + 1e-6)\n",
        "\n",
        "    if rate < 1:\n",
        "        for grad in grads :\n",
        "            grad *= rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_M4GnPUGLlN"
      },
      "source": [
        "# LSTM 클래스 구현\n",
        "\n",
        "class LSTM:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, H = h_prev.shape\n",
        "\n",
        "        A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b\n",
        "\n",
        "        #slice\n",
        "        f = A[:, :H]\n",
        "        g = A[:, H:2*H]\n",
        "        i = A[:, 2*H:3*H]\n",
        "        o = A[:, 3*H:]\n",
        "\n",
        "        f = sigmoid(f)\n",
        "        g = np.tanh(g)\n",
        "        i = sigmoid(i)\n",
        "        o = sigmoid(o)\n",
        "\n",
        "        c_next = f * c_prev + g * i\n",
        "        h_next = o * np.tanh(c_next)\n",
        "\n",
        "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
        "        return h_next, c_next"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os4Sj9F4JJFm"
      },
      "source": [
        "# TimeLSTM 구현\n",
        "\n",
        "class TimeLSTM:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.c = None, None\n",
        "        self.dh = None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        H = Wh.shape[0]\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "        if not self.stateful or self.c is None:\n",
        "            self.c = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = LSTM(*self.params)\n",
        "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
        "            hs[:, t, :] = self.h\n",
        "\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D = Wx.shape[0]\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh, dc = 0, 0\n",
        "\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
        "            dxs[:, t, :] = dx\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h, c=None):\n",
        "        self.h, self.c = h, c\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h, self.c = None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuBaSmbEKF0-"
      },
      "source": [
        "# Rnnlm 클래스 구현\n",
        "\n",
        "\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.time_layers import *\n",
        "from common.base_model import BaseModel\n",
        "\n",
        "\n",
        "class Rnnlm(BaseModel):\n",
        "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # 가중치 초기화\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b)\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.lstm_layer = self.layers[1]\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        return xs\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        score = self.predict(xs)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.lstm_layer.reset_state()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd7PsugxLhmP"
      },
      "source": [
        "# BetterRnnlm 클래스 구현\n",
        "\n",
        "\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.time_layers import *\n",
        "from common.base_model import BaseModel\n",
        "from common.np import *\n",
        "\n",
        "class BetterRnnlm(BaseModel):\n",
        "    def __init__(self, vocab_size=10000, wordvec_size=650, hidden_size=650, dropout_ratio=0.5):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "        \n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
        "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # 세 가지 개선!\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeAffine(embed_W.T, affine_b)  # 가중치 공유\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
        "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs, train_flg=False):\n",
        "        for layer in self.drop_layers:\n",
        "            layer.train_flg = train_flg\n",
        "\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        return xs\n",
        "\n",
        "    def forward(self, xs, ts, train_flg=True):\n",
        "        score = self.predict(xs, train_flg)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        for layer in self.lstm_layers:\n",
        "            layer.reset_state()       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E248Yyn6Lks2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "ad2e1666-32b6-4b8d-bf07-e78daebdbbf2"
      },
      "source": [
        "# Better Rnnlm 학습시키기\n",
        "# cpu 2일, gpu 5시간 소요\n",
        "'''\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common import config\n",
        "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
        "# ==============================================\n",
        "# config.GPU = True\n",
        "# ==============================================\n",
        "from common.optimizer import SGD\n",
        "from common.trainer import RnnlmTrainer\n",
        "from common.util import eval_perplexity, to_gpu\n",
        "from dataset import ptb\n",
        "from ch06.better_rnnlm import BetterRnnlm\n",
        "\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 20\n",
        "wordvec_size = 650\n",
        "hidden_size = 650\n",
        "time_size = 35\n",
        "lr = 20.0\n",
        "max_epoch = 40\n",
        "max_grad = 0.25\n",
        "dropout = 0.5\n",
        "\n",
        "# 학습 데이터 읽기\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "corpus_val, _, _ = ptb.load_data('val')\n",
        "corpus_test, _, _ = ptb.load_data('test')\n",
        "\n",
        "if config.GPU:\n",
        "    corpus = to_gpu(corpus)\n",
        "    corpus_val = to_gpu(corpus_val)\n",
        "    corpus_test = to_gpu(corpus_test)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "xs = corpus[:-1]\n",
        "ts = corpus[1:]\n",
        "\n",
        "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\n",
        "optimizer = SGD(lr)\n",
        "trainer = RnnlmTrainer(model, optimizer)\n",
        "\n",
        "best_ppl = float('inf')\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n",
        "                time_size=time_size, max_grad=max_grad)\n",
        "\n",
        "    model.reset_state()\n",
        "    ppl = eval_perplexity(model, corpus_val)\n",
        "    print('검증 퍼플렉서티: ', ppl)\n",
        "\n",
        "    if best_ppl > ppl:\n",
        "        best_ppl = ppl\n",
        "        model.save_params()\n",
        "    else:\n",
        "        lr /= 4.0\n",
        "        optimizer.lr = lr\n",
        "\n",
        "    model.reset_state()\n",
        "    print('-' * 50)\n",
        "\n",
        "\n",
        "# 테스트 데이터로 평가\n",
        "model.reset_state()\n",
        "ppl_test = eval_perplexity(model, corpus_test)\n",
        "print('테스트 퍼플렉서티: ', ppl_test)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport sys, os\\nos.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\\nsys.path.append(os.chdir)\\nfrom common import config\\n# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\\n# ==============================================\\n# config.GPU = True\\n# ==============================================\\nfrom common.optimizer import SGD\\nfrom common.trainer import RnnlmTrainer\\nfrom common.util import eval_perplexity, to_gpu\\nfrom dataset import ptb\\nfrom ch06.better_rnnlm import BetterRnnlm\\n\\n\\n# 하이퍼파라미터 설정\\nbatch_size = 20\\nwordvec_size = 650\\nhidden_size = 650\\ntime_size = 35\\nlr = 20.0\\nmax_epoch = 40\\nmax_grad = 0.25\\ndropout = 0.5\\n\\n# 학습 데이터 읽기\\ncorpus, word_to_id, id_to_word = ptb.load_data(\\'train\\')\\ncorpus_val, _, _ = ptb.load_data(\\'val\\')\\ncorpus_test, _, _ = ptb.load_data(\\'test\\')\\n\\nif config.GPU:\\n    corpus = to_gpu(corpus)\\n    corpus_val = to_gpu(corpus_val)\\n    corpus_test = to_gpu(corpus_test)\\n\\nvocab_size = len(word_to_id)\\nxs = corpus[:-1]\\nts = corpus[1:]\\n\\nmodel = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\\noptimizer = SGD(lr)\\ntrainer = RnnlmTrainer(model, optimizer)\\n\\nbest_ppl = float(\\'inf\\')\\nfor epoch in range(max_epoch):\\n    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\\n                time_size=time_size, max_grad=max_grad)\\n\\n    model.reset_state()\\n    ppl = eval_perplexity(model, corpus_val)\\n    print(\\'검증 퍼플렉서티: \\', ppl)\\n\\n    if best_ppl > ppl:\\n        best_ppl = ppl\\n        model.save_params()\\n    else:\\n        lr /= 4.0\\n        optimizer.lr = lr\\n\\n    model.reset_state()\\n    print(\\'-\\' * 50)\\n\\n\\n# 테스트 데이터로 평가\\nmodel.reset_state()\\nppl_test = eval_perplexity(model, corpus_test)\\nprint(\\'테스트 퍼플렉서티: \\', ppl_test)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyD3k7QJEM36"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "import numpy as np\n",
        "from common.functions import softmax\n",
        "from ch06.rnnlm import Rnnlm\n",
        "from ch06.better_rnnlm import BetterRnnlm\n",
        "\n",
        "class RnnlmGen(Rnnlm):\n",
        "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
        "        word_ids = [start_id]\n",
        "\n",
        "        x = start_id\n",
        "        while len(word_ids) < sample_size:\n",
        "            x = np.array(x).reshape(1, 1)\n",
        "            score = self.predict(x)\n",
        "            p = softmax(score.flatten())\n",
        "\n",
        "            sampled = np.random.choice(len(p), size = 1, p=p)\n",
        "            if (skip_ids is None) or (Sampled not in skip_ids):\n",
        "                x = sampled\n",
        "                word_ids.append(int(x))\n",
        "\n",
        "        return word_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aB0qPR-NGtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cedf77c1-a617-45fc-b4cb-6fa349b18f3e"
      },
      "source": [
        "# RnnlmGen 클래스를 사용해 문장 생성해보기\n",
        "\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from ch07.rnnlm_gen import RnnlmGen\n",
        "from dataset import ptb\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "model = RnnlmGen()\n",
        "# 밑에거 주석 해제하면 학습 끝낸 가중치 이용\n",
        "# model.load_params('/content/drive/My Drive/deep-learning-from-scratch-2-master/ch06/Rnnlm.pkl') \n",
        "\n",
        "# 시작(start) 문자와 건너뜀(skip) 문자 설정\n",
        "start_word = 'you'\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = ['N', '<unk>', '$']\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "\n",
        "# 문장 생성\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos> ', '.\\n')\n",
        "print(txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you european shifting leval unscrupulous produced judicial day plaza petrochemical restricts scientist madrid montgomery salespeople upgrading sci felt yearly acres eidsmo finding polyethylene contrasts olympia announcing ill besides closed lazard reebok grenfell alaskan lead dun turns misleading domestically lab aging founder terminals outstanding tandem worry sorts ross mattel nuovo shifts networks habit cineplex brady dive possibilities lauder ousted desert whole reduces oral clara moved earthquake bearish confronted furs sharon army fidelity unfairly guilty single welch petrochemicals sr. accessories rather campaign stopped pride stein wachovia misstated capel beta sets dispatched corning enemy backer tenure translated epa might third-largest massachusetts role hollander\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fB9zzh_UqSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b367b984-b123-4b4e-cd76-737e397f143c"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.np import *\n",
        "from ch07.rnnlm_gen import BetterRnnlmGen\n",
        "from dataset import ptb\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "\n",
        "model = BetterRnnlmGen()\n",
        "model.load_params('/content/drive/My Drive/deep-learning-from-scratch-2-master/ch06/BetterRnnlm.pkl')\n",
        "\n",
        "# start 문자와 skip 문자 설정\n",
        "start_word = 'you'\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = ['N', '<unk>', '$']\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "# 문장 생성\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "\n",
        "print(txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you had one but classic a lot of italy corporate mail in the country and mr. shore up european and overwhelming bias in this category to that tobacco business is diverted from expenses in the u.s..\n",
            " the citizens ' assets are provided for a similar collection.\n",
            " their plan are copy of a feelings to trial black and political opponents since the least short of the most part of the cowboys published at the western government.\n",
            " for a recent to command the industry in june the farm interest is learning for three years.\n",
            " t. rowe price as\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA--6VN9toQR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "757b9065-f75c-47ef-e960-9b8bf0876dcc"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.np import *\n",
        "from ch07.rnnlm_gen import BetterRnnlmGen\n",
        "from dataset import ptb\n",
        "\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "\n",
        "model = BetterRnnlmGen()\n",
        "model.load_params('/content/drive/My Drive/deep-learning-from-scratch-2-master/ch06/BetterRnnlm.pkl')\n",
        "\n",
        "# start 문자와 skip 문자 설정\n",
        "\n",
        "start_word = 'you'\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = ['N', '<unk>', '$']\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "# 문장 생성\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "\n",
        "print(txt)\n",
        "\n",
        "'''\n",
        "model.reset_state()\n",
        "\n",
        "start_words = 'the meaning of life is'\n",
        "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
        "\n",
        "for x in start_ids[:-1]:\n",
        "    x = np.array(x).reshape(1, 1)\n",
        "    model.predict(x)\n",
        "\n",
        "word_ids = model.generate(start_ids[-1], skip_ids)\n",
        "word_ids = start_ids[:-1] + word_ids\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n'\n",
        "print(txt)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you may concede the industry is learning reruns not to defend and wait its firm right away to everything.\n",
            " i told anybody that any of the controversy.\n",
            " now the people are certainly getting much practical and the congressman says that for the time dependents is bets on the desire and he is charts.\n",
            " but first two studies were scheduled he asked on such couples as many killings as a. gordon conn. was named chairman and chief executive.\n",
            " charles n. smith frank martin a distinguished rothschild vice president of st. louis and frederick thompson.\n",
            " the appointments\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nmodel.reset_state()\\n\\nstart_words = 'the meaning of life is'\\nstart_ids = [word_to_id[w] for w in start_words.split(' ')]\\n\\nfor x in start_ids[:-1]:\\n    x = np.array(x).reshape(1, 1)\\n    model.predict(x)\\n\\nword_ids = model.generate(start_ids[-1], skip_ids)\\nword_ids = start_ids[:-1] + word_ids\\ntxt = ' '.join([id_to_word[i] for i in word_ids])\\ntxt = txt.replace(' <eos>', '.\\n'\\nprint(txt)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgLxAM16vSmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac1e750-e3a7-435d-bc52-52c25adcbee7"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from dataset import sequence\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt', seed=1984)\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "print(x_train.shape, t_train.shape)\n",
        "print(x_test.shape, t_test.shape)\n",
        "\n",
        "print(x_train[0])\n",
        "print(t_train[0])\n",
        "\n",
        "print(''.join([id_to_char[c] for c in x_train[0]]))\n",
        "print(''.join([id_to_char[c] for c in t_train[0]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 7) (45000, 5)\n",
            "(5000, 7) (5000, 5)\n",
            "[ 3  0  2  0  0 11  5]\n",
            "[ 6  0 11  7  5]\n",
            "71+118 \n",
            "_189 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbwu7y6Wz5j_"
      },
      "source": [
        "# Encoder 클래스 구현\n",
        "class Encoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D)/ 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful-False)\n",
        "\n",
        "        self.params = self.embed.params + self.lstm.params\n",
        "        self.grads = self.embed.grads + self.lstm.grads\n",
        "        self.hs = None\n",
        "\n",
        "        def forward(self, xs):\n",
        "            xs = self.embed.forward(xs)\n",
        "            hs = self.lstm.forward(xs)\n",
        "            self.hs = hs\n",
        "            return hs[:, -1, :]\n",
        "\n",
        "        def backward(self, dh):\n",
        "            dhs = np.zeros_like(self.hs)\n",
        "            dhs[:, -1, :] = dh\n",
        "\n",
        "            dout = self.lstm.backward(dhs)\n",
        "            dout = self.embed.backward(dout)\n",
        "            return dout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-NncGN13Lgt"
      },
      "source": [
        "# Decoder 클래스 구현\n",
        "class Decoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D)/ 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful-True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "    \n",
        "    def forward(self, xs, h):\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        out = self.lstm.forward(out)\n",
        "        score = self.affine.forward(out)\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dout = self.embed.backward(dout)\n",
        "        dh = self.lstm.dh\n",
        "        return dh\n",
        "\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array(sample_id).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "            out = self.lstm.forward(out)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(int(sample_id))\n",
        "\n",
        "        return sampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZzV-PSr6rzj"
      },
      "source": [
        "# seq2seq 클래스 구현\n",
        "\n",
        "class Seq2seq(BaseModel):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = Decoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
        "        h = self.encoder.forward(xs)\n",
        "        score = self.decoder.forward(decoder_xs, h)\n",
        "        loss = self.softmax.forward(score, decoder_ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.softmax.backward(dout)\n",
        "        dh = self.decoder.backward(dout)\n",
        "        dout = self.encoder.backward(dh)\n",
        "        return dout\n",
        "\n",
        "    def generate(self, xs, start_id, sample_size):\n",
        "        h = self.encoder.forward(xs)\n",
        "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
        "        return sampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT7pULPYADDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c2e66b-22ac-4a86-9d52-5d85df1b6452"
      },
      "source": [
        "['_', '6', '2', '', '']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_', '6', '2', '', '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc2bkeUdkQYC"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Seq2seq, Encoder\n",
        "\n",
        "\n",
        "class PeekyDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, xs, h):\n",
        "        N, T = xs.shape\n",
        "        N, H = h.shape\n",
        "\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        out = self.lstm.forward(out)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        score = self.affine.forward(out)\n",
        "        self.cache = H\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        H = self.cache\n",
        "\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
        "        self.embed.backward(dembed)\n",
        "\n",
        "        dhs = dhs0 + dhs1\n",
        "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
        "        return dh\n",
        "\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        char_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        H = h.shape[1]\n",
        "        peeky_h = h.reshape(1, 1, H)\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([char_id]).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            out = self.lstm.forward(out)\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            char_id = np.argmax(score.flatten())\n",
        "            sampled.append(char_id)\n",
        "\n",
        "        return sampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7BUYvShojCu"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Seq2seq, Encoder\n",
        "\n",
        "class PeekySeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = PeekyDecoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji6Et5R9wE8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86d2b17-2536-4155-9b80-4375c891e200"
      },
      "source": [
        "#어텐션 \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "T, H = 5, 4 # 형상 설정\n",
        "hs = np.random.randn(T,H)\n",
        "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02]) # 가중치\n",
        "\n",
        "ar = a.reshape(5,1).repeat(4, axis = 1)\n",
        "print(ar.shape)\n",
        "\n",
        "t = hs * ar\n",
        "print(t.shape)\n",
        "\n",
        "c = np.sum(t, axis = 0)\n",
        "print(c.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 4)\n",
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbAeHv8F8fzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c1049c-080f-43eb-ce09-91b53ebc4a5d"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "os.chdir('/content/drive/My Drive/deep-learning-from-scratch-2-master')\n",
        "sys.path.append('os.chdir')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "from ch08.attention_seq2seq import AttentionSeq2seq\n",
        "from ch07.seq2seq import Seq2seq\n",
        "from ch07.peeky_seq2seq import PeekySeq2seq\n",
        "\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 입력 문장 반전\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 256\n",
        "batch_size = 128\n",
        "max_epoch = 10\n",
        "max_grad = 5.0\n",
        "\n",
        "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "acc_list = []\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(x_train, t_train, max_epoch=1,\n",
        "                batch_size=batch_size, max_grad=max_grad)\n",
        "\n",
        "    correct_num = 0\n",
        "    for i in range(len(x_test)):\n",
        "        question, correct = x_test[[i]], t_test[[i]]\n",
        "        verbose = i < 10\n",
        "        correct_num += eval_seq2seq(model, question, correct,\n",
        "                                    id_to_char, verbose, is_reverse=True)\n",
        "\n",
        "    acc = float(correct_num) / len(x_test)\n",
        "    acc_list.append(acc)\n",
        "    print('정확도 %.3f%%' % (acc * 100))\n",
        "\n",
        "\n",
        "model.save_params()\n",
        "\n",
        "# 그래프 그리기\n",
        "x = np.arange(len(acc_list))\n",
        "plt.plot(x, acc_list, marker='o')\n",
        "plt.xlabel('에폭')\n",
        "plt.ylabel('정확도')\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.10\n",
            "| 에폭 1 |  반복 21 / 351 | 시간 13[s] | 손실 3.12\n",
            "| 에폭 1 |  반복 41 / 351 | 시간 27[s] | 손실 1.91\n",
            "| 에폭 1 |  반복 61 / 351 | 시간 40[s] | 손실 1.75\n",
            "| 에폭 1 |  반복 81 / 351 | 시간 54[s] | 손실 1.54\n",
            "| 에폭 1 |  반복 101 / 351 | 시간 68[s] | 손실 1.21\n",
            "| 에폭 1 |  반복 121 / 351 | 시간 81[s] | 손실 1.15\n",
            "| 에폭 1 |  반복 141 / 351 | 시간 95[s] | 손실 1.10\n",
            "| 에폭 1 |  반복 161 / 351 | 시간 108[s] | 손실 1.07\n",
            "| 에폭 1 |  반복 181 / 351 | 시간 122[s] | 손실 1.05\n",
            "| 에폭 1 |  반복 201 / 351 | 시간 136[s] | 손실 1.04\n",
            "| 에폭 1 |  반복 221 / 351 | 시간 149[s] | 손실 1.03\n",
            "| 에폭 1 |  반복 241 / 351 | 시간 163[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 261 / 351 | 시간 176[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 281 / 351 | 시간 190[s] | 손실 1.01\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}